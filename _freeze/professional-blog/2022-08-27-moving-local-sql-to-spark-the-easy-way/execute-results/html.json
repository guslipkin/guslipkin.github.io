{
  "hash": "bbccdb580d67869a70c8ff2d625d0ef0",
  "result": {
    "markdown": "---\ntitle: \"Moving Local Data Pipelines to Spark: The Easy Way with R and Python\"\ndescription: \"It might not be easy, but it'll certainly be less painful\"\nimage: \"../assets/post-assets/2022-08-27-moving-local-sql-to-spark-the-easy-way/Little_Green_Men.jpg\"\nauthor: \"Gus Lipkin\"\ndate: \"2022/08/27\"\ncategories:\n  - R\n  - \"R: sparklyr\"\n  - Python\n  - \"Python: PySpark\"\n  - Spark\nformat:\n  html:\n    theme: default\n    toc: true\n---\n\n\n::: {.callout-note}\n\nData comes from the [*Olympic Historical Dataset From Olympedia.org*](https://www.kaggle.com/datasets/josephcheng123456/olympic-historical-dataset-from-olympediaorg)\n\n:::\n\n# Intro\n\nAs data people, we know that \"the cloud\" is usually a server somewhere maybe hosted on Azure, a server in a closet, or maybe a ten year old laptop underneath someone's desk. When we hear people ask about moving data and processes to the cloud, it's hard not to think of the [Little Green Men](https://disney.fandom.com/wiki/Little_Green_Men) from Toy Story worshiping \"The Claw\". Within a few short breaths, you've been asked to try and move all your current processes to the cloud. It's a daunting task, but with a few handy tricks, we can make the SQL conversion relatively painless.\n\n![](../assets/post-assets/2022-08-27-moving-local-sql-to-spark-the-easy-way/Little_Green_Men.jpg){fig-alt=\"Three Little Green Men from Toy Story saying 'The Cloud'\"}\n\n# Getting Started\n\n::: {.callout-tip}\n\nI've generally tried to keep the R and Python code as similar as possible, but that's not always the wisest move. If you have any questions, don't hesitate to reach out.\n\n:::\n\nOur first step is to load our packages and connect to spark. We'll also create a local spark instance to use rather than link to a server. On the R side of things, our core packages are `sparklyr` and `glue` while Python is using `pyspark` and Python 3's f-String functionality.\n\n::: {.panel-tabset}\n\n##### `R`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/LibrariesConnect.R_af4f04c81b5a9261b1db97dadb6ed84d'}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nlibrary(glue)\n\nsc <- spark_connect(master = \"local\")\n```\n:::\n\n\n##### `Python`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/LibrariesConnect.py_f0aca692b05276cfbccce0a9c4713d1a'}\n\n```{.python .cell-code}\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nsc = SparkSession.builder.getOrCreate()\n```\n:::\n\n\n:::\n\n# Prepping the Data\n\nAt this point \"in the real world\" you'll have loaded your data into the table format of your choosing. Where I work, all of our tables are in delta or parquet formats. For this example, rather than create dummy data for a post like I usually do, I've downloaded the [*Olympic Historical Dataset From Olympedia.org*](https://www.kaggle.com/datasets/josephcheng123456/olympic-historical-dataset-from-olympediaorg) from Kaggle and moved everything to my data folder.\n\n::: {.panel-tabset}\n\n##### `R`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/ReadCSV.R_6763e6e51db196b0b7523e46bdafa50d'}\n\n```{.r .cell-code}\ntbl_athlete_bio <- \n  spark_read_csv(sc, name = \"athlete_bio\",\n                 path = \"../assets/data/Olympic_Athlete_Bio.csv\")\n\ntbl_athlete_results <- \n  spark_read_csv(sc, name = \"athlete_results\", \n                 path = \"../assets/data/Olympic_Athlete_Event_Results.csv\")\n\ntbl_results <- \n  spark_read_csv(sc, name = \"results\",\n                 path = \"../assets/data/Olympic_Athlete_Event_Results.csv\")\n\ntbl_medal_tally <- \n  spark_read_csv(sc, name = \"medal_tally\",\n                 path = \"../assets/data/Olympic_Games_Medal_Tally.csv\")\n\ntbl_games <- \n  spark_read_csv(sc, name = \"games\",\n                 path = \"../assets/data/Olympics_Games.csv\")\n\ntbl_country <- spark_read_csv(sc, name = \"country\",\n                              path = \"../assets/data/Olympics_Country.csv\")\n```\n:::\n\n\n##### `Python`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/ReadCSV.py_f07d398b8a3dc80b706dd360288ed8cb'}\n\n```{.python .cell-code}\ntbl_athlete_bio = sc.read.csv(\n  \"../assets/data/Olympic_Athlete_Bio.csv\", header = True)\ntbl_athlete_bio.createOrReplaceTempView(\"athlete_bio\")\n\ntbl_athlete_results = sc.read.csv(\n    \"../assets/data/Olympic_Athlete_Event_Results.csv\", header = True)\ntbl_athlete_results.createOrReplaceTempView(\"athlete_results\")\n\ntbl_results = sc.read.csv(\n  \"../assets/data/Olympic_Athlete_Event_Results.csv\", header = True)\ntbl_results.createOrReplaceTempView(\"results\")\n\ntbl_medal_tally = sc.read.csv(\n  \"../assets/data/Olympic_Games_Medal_Tally.csv\", header = True)\ntbl_medal_tally.createOrReplaceTempView(\"medal_tally\")\n\ntbl_games = sc.read.csv(\n  \"../assets/data/Olympics_Games.csv\", header = True)\ntbl_games.createOrReplaceTempView(\"games\")\n\ntbl_country = sc.read.csv(\n  \"../assets/data/Olympics_Country.csv\", header = True)\ntbl_country.createOrReplaceTempView(\"country\")\n```\n:::\n\n\n:::\n\nBy assigning the tables to both an R and Python variable and a name in Spark, we're able to access the data from both an R and Python context, and a SQL context.\n\n::: {.panel-tabset}\n\n##### `R`\n\n\n::: {.cell layout-ncol=\"2\" tbl-subcap='[\"sparklyr\",\"SQL\"]' hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/PreviewTable.R_810dff5d6c5d6eff9f0c2fa085adc46f'}\n\n```{.r .cell-code}\n# R\nhead(tbl_country)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source: spark<?> [?? x 2]\n  country_noc country       \n  <chr>       <chr>         \n1 AFG         Afghanistan   \n2 ALB         Albania       \n3 ALG         Algeria       \n4 ASA         American Samoa\n5 AND         Andorra       \n6 ANG         Angola        \n```\n:::\n\n```{.r .cell-code}\n# SQL\nsdf_sql(sc, 'SELECT * FROM country LIMIT 6')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source: spark<?> [?? x 2]\n  country_noc country       \n  <chr>       <chr>         \n1 AFG         Afghanistan   \n2 ALB         Albania       \n3 ALG         Algeria       \n4 ASA         American Samoa\n5 AND         Andorra       \n6 ANG         Angola        \n```\n:::\n:::\n\n\n##### `Python`\n\n\n::: {.cell layout-ncol=\"2\" tbl-subcap='[\"PySpark\",\"SQL\"]' hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/PreviewTable.py_7a571b644843ab35ea4a66518b160638'}\n\n```{.python .cell-code}\n# Python\ntbl_country.show(6)\n# SQL\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-----------+--------------+\n|country_noc|       country|\n+-----------+--------------+\n|        AFG|   Afghanistan|\n|        ALB|       Albania|\n|        ALG|       Algeria|\n|        ASA|American Samoa|\n|        AND|       Andorra|\n|        ANG|        Angola|\n+-----------+--------------+\nonly showing top 6 rows\n```\n:::\n\n```{.python .cell-code}\nsc.sql('SELECT * FROM country LIMIT 6').show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-----------+--------------+\n|country_noc|       country|\n+-----------+--------------+\n|        AFG|   Afghanistan|\n|        ALB|       Albania|\n|        ALG|       Algeria|\n|        ASA|American Samoa|\n|        AND|       Andorra|\n|        ANG|        Angola|\n+-----------+--------------+\n```\n:::\n:::\n\n\n:::\n\n# Working With Views\n\nLet's say your database administrators (if they aren't also you) are kind and have done a small amount of data cleaning and you usually access the data from a mount point which provides a view. You pull the schema for the `athlete_bio_vw` table and get the following SQL that references `athlete_bio`.\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/View.sql_546c8e682bfdbe4aa920349907772eb9'}\n\n```{.sql .cell-code}\nSELECT \n  athlete_id, name, sex, CAST(born AS DATE), \n  CAST(height AS DOUBLE), CAST(weight AS DOUBLE), \n  country, country_noc\nFROM athlete_bio \nWHERE \n  height != \"na\" AND\n  weight != \"na\"\n```\n:::\n\n\nWe can re-create the views three ways. We can use R or Python to recreate the view from scratch by referencing the table variable name, not the internal Spark name. We can also wrap a SQL statement in our language of choice and use the internal Spark name. We then assign this new data frame to `tbl_athlete_bio_vw`. However, because the new data frame has been assigned as a variable, we no longer have direct and easy access to the view inside further SQL queries, and would have to use R or Python to do any more analysis.\n\n::: {.panel-tabset}\n\n##### `R`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/View.R_a39b05d6b4823cb2ab9b0e0f49801214'}\n\n```{.r .cell-code}\n# R\ntbl_athlete_bio_vw <-\n  tbl_athlete_bio |> \n    filter(height != \"na\", weight != \"na\") |>\n    mutate(born = as.Date(born), \n           height = as.double(height), \n           weight = as.double(weight)) |>\n    select(athlete_id, name, sex, born, height, weight, country, country_noc)\n# SQL\ntbl_athlete_bio_vw <- sdf_sql(sc, '\n  SELECT \n    athlete_id, name, sex, CAST(born AS DATE), CAST(height AS DOUBLE), \n    CAST(weight AS DOUBLE), country, country_noc\n  FROM athlete_bio \n  WHERE \n    height != \"na\" AND\n    weight != \"na\"')\n\nhead(tbl_athlete_bio_vw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source: spark<?> [?? x 8]\n  athlete_id name                sex    born       height weight country count…¹\n       <int> <chr>               <chr>  <date>      <dbl>  <dbl> <chr>   <chr>  \n1      43737 Andrzej Socharski   Male   1947-08-31    173     72 \" Pola… POL    \n2      50147 Nathalie Wunderlich Female 1971-06-03    170     50 \" Swit… SUI    \n3       5085 Miha Lokar          Male   1935-09-10    182     76 \" Yugo… YUG    \n4     136329 Austin Hack         Male   1992-05-17    203    100 \" Unit… USA    \n5      38633 Tsuneo Ogasawara    Male   1942-07-30    181     80 \" Japa… JPN    \n6      77095 Fulgence Rwabu      Male   1947-11-23    165     51 \" Ugan… UGA    \n# … with abbreviated variable name ¹​country_noc\n```\n:::\n:::\n\n\n##### `Python`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/View.py_93f9ab07a22c6dfd33cbd32a7ba24908'}\n\n```{.python .cell-code}\n# Python\ntbl_athlete_bio_vw = tbl_athlete_bio.filter('height != \"na\" AND weight != \"na\"'\n    ).select(['athlete_id', 'name', 'sex', col('born').cast('date'), \n              col('height').cast('double'), col('weight').cast('double'), \n              'country', 'country_noc'])\n# SQL\ntbl_athlete_bio_vw = sc.sql('''\n  SELECT \n    athlete_id, name, sex, CAST(born AS DATE), CAST(height AS DOUBLE), \n    CAST(weight AS DOUBLE), country, country_noc\n  FROM athlete_bio \n  WHERE \n    height != \"na\" AND\n    weight != \"na\"''')\n    \ntbl_athlete_bio_vw.show(6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+-------------------+------+----------+------+------+--------------+-----------+\n|athlete_id|               name|   sex|      born|height|weight|       country|country_noc|\n+----------+-------------------+------+----------+------+------+--------------+-----------+\n|     43737|  Andrzej Socharski|  Male|1947-08-31| 173.0|  72.0|        Poland|        POL|\n|     50147|Nathalie Wunderlich|Female|1971-06-03| 170.0|  50.0|   Switzerland|        SUI|\n|      5085|         Miha Lokar|  Male|1935-09-10| 182.0|  76.0|    Yugoslavia|        YUG|\n|    136329|        Austin Hack|  Male|1992-05-17| 203.0| 100.0| United States|        USA|\n|     38633|   Tsuneo Ogasawara|  Male|1942-07-30| 181.0|  80.0|         Japan|        JPN|\n|     77095|     Fulgence Rwabu|  Male|1947-11-23| 165.0|  51.0|        Uganda|        UGA|\n+----------+-------------------+------+----------+------+------+--------------+-----------+\nonly showing top 6 rows\n```\n:::\n:::\n\n\n:::\n\nThe third way is a little chaotic, but is actually my preferred method because I can save the queries I want to use as subqueries later on. First we save the query as a string and then use the R `{glue}` package or f-strings from Python to run the SQL query in the `tbl_athlete_bio_qry` variable. Because this method lends itself to subqueries so well, I don't usually use it unless I intend to use the query later on.\n\nIf you're not familiar, both `glue` and f-strings take a regular string and insert the value of the variable inside curly braces `{}`. For example, if you have a variable named `name` that has someone's name. You can use `glue` or f-strings to write `\"hello {name}\"` and when this is evaluated, if the name is Gus, it will print \"hello Gus.\"\n\n::: {.panel-tabset}\n\n##### `R`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/ViewQuery.R_8ee97a475ff6a6909bbda8be5567fd4e'}\n\n```{.r .cell-code}\nathlete_bio_vw_qry <- '\n  SELECT \n    athlete_id, name, sex, CAST(born AS DATE), CAST(height AS DOUBLE), \n    CAST(weight AS DOUBLE), country, country_noc\n  FROM athlete_bio \n  WHERE \n    height != \"na\" AND\n    weight != \"na\"'\ntbl_athlete_bio_vw <- sdf_sql(sc, glue(\"{athlete_bio_vw_qry}\"))\n\nhead(tbl_athlete_bio_vw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source: spark<?> [?? x 8]\n  athlete_id name                sex    born       height weight country count…¹\n       <int> <chr>               <chr>  <date>      <dbl>  <dbl> <chr>   <chr>  \n1      43737 Andrzej Socharski   Male   1947-08-31    173     72 \" Pola… POL    \n2      50147 Nathalie Wunderlich Female 1971-06-03    170     50 \" Swit… SUI    \n3       5085 Miha Lokar          Male   1935-09-10    182     76 \" Yugo… YUG    \n4     136329 Austin Hack         Male   1992-05-17    203    100 \" Unit… USA    \n5      38633 Tsuneo Ogasawara    Male   1942-07-30    181     80 \" Japa… JPN    \n6      77095 Fulgence Rwabu      Male   1947-11-23    165     51 \" Ugan… UGA    \n# … with abbreviated variable name ¹​country_noc\n```\n:::\n:::\n\n\n##### `Python`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/ViewQuery.py_77be1297253f079be5ee27dccef5f939'}\n\n```{.python .cell-code}\nathlete_bio_vw_qry = '''\n  SELECT \n    athlete_id, name, sex, CAST(born AS DATE), CAST(height AS DOUBLE), \n    CAST(weight AS DOUBLE), country, country_noc\n  FROM athlete_bio \n  WHERE \n    height != \"na\" AND\n    weight != \"na\"'''\ntbl_athlete_bio_vw = sc.sql(f'''{athlete_bio_vw_qry}''')\n\ntbl_athlete_bio_vw.show(6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+-------------------+------+----------+------+------+--------------+-----------+\n|athlete_id|               name|   sex|      born|height|weight|       country|country_noc|\n+----------+-------------------+------+----------+------+------+--------------+-----------+\n|     43737|  Andrzej Socharski|  Male|1947-08-31| 173.0|  72.0|        Poland|        POL|\n|     50147|Nathalie Wunderlich|Female|1971-06-03| 170.0|  50.0|   Switzerland|        SUI|\n|      5085|         Miha Lokar|  Male|1935-09-10| 182.0|  76.0|    Yugoslavia|        YUG|\n|    136329|        Austin Hack|  Male|1992-05-17| 203.0| 100.0| United States|        USA|\n|     38633|   Tsuneo Ogasawara|  Male|1942-07-30| 181.0|  80.0|         Japan|        JPN|\n|     77095|     Fulgence Rwabu|  Male|1947-11-23| 165.0|  51.0|        Uganda|        UGA|\n+----------+-------------------+------+----------+------+------+--------------+-----------+\nonly showing top 6 rows\n```\n:::\n:::\n\n\n:::\n\n# Subqueries\n\nYou probably know that the idea behind subqueries is that it lets you nest operations. The idea behind my third method (I really need a better name for it) is the same. Rather, instead of nesting queries directly, we're nesting strings which are then evaluated as queries. In this example, we're going to make another subquery called `tbl_athlete_results_qry` that contains all columns from the `athlete_results` Spark table for medal winners. We then want to return all rows from `tbl_athlete_bio_vw` for people who have won medals.\n\n::: {.panel-tabset}\n\n##### `R`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/Subquery.R_7bff04ac0fe340d18e4ded2a71bd4c92'}\n\n```{.r .cell-code}\nathlete_results_qry <- '\n  SELECT athlete_id \n  FROM athlete_results\n  WHERE medal != \"na\"'\nsdf_sql(sc, glue('\n  SELECT *\n  FROM ({athlete_bio_vw_qry})\n  WHERE athlete_id IN ({athlete_results_qry})')) |>\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source: spark<?> [?? x 8]\n  athlete_id name             sex    born       height weight country    count…¹\n       <int> <chr>            <chr>  <date>      <dbl>  <dbl> <chr>      <chr>  \n1    1700071 Lee Myung-Hwa    Female 1964-09-07    168     57 \" Republi… KOR    \n2      58599 István Kozma     Male   1939-11-27    198    125 \" Hungary\" HUN    \n3      31969 Valter Matošević Male   1970-06-11    194     99 \" Croatia\" CRO    \n4      91237 Volha Puzhevich  Female 1983-03-17    167     43 \" Belarus\" BLR    \n5     104818 Marko Kemppainen Male   1976-07-13    184    100 \" Finland\" FIN    \n6       5695 Lew Beck         Male   1922-04-19    183     75 \" United … USA    \n# … with abbreviated variable name ¹​country_noc\n```\n:::\n:::\n\n\n##### `Python`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/Subquery.py_ebd6f2d8e1d5dcb59feb2546d462219f'}\n\n```{.python .cell-code}\nathlete_results_qry = '''\n  SELECT athlete_id \n  FROM athlete_results\n  WHERE medal != \"na\"'''\nsc.sql(f'''\n  SELECT *\n  FROM ({athlete_bio_vw_qry})\n  WHERE athlete_id IN ({athlete_results_qry})''').show(6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+----------------+------+----------+------+------+------------------+-----------+\n|athlete_id|            name|   sex|      born|height|weight|           country|country_noc|\n+----------+----------------+------+----------+------+------+------------------+-----------+\n|   1700071|   Lee Myung-Hwa|Female|1964-09-07| 168.0|  57.0| Republic of Korea|        KOR|\n|     58599|    István Kozma|  Male|1939-11-27| 198.0| 125.0|           Hungary|        HUN|\n|     31969|Valter Matošević|  Male|1970-06-11| 194.0|  99.0|           Croatia|        CRO|\n|     91237| Volha Puzhevich|Female|1983-03-17| 167.0|  43.0|           Belarus|        BLR|\n|    104818|Marko Kemppainen|  Male|1976-07-13| 184.0| 100.0|           Finland|        FIN|\n|      5695|        Lew Beck|  Male|1922-04-19| 183.0|  75.0|     United States|        USA|\n+----------+----------------+------+----------+------+------+------------------+-----------+\nonly showing top 6 rows\n```\n:::\n:::\n\n\n:::\n\nIf, for whatever reason, we wanted to go even deeper with the subqueries, we could. All we have to do is make sure we create our smallest level queries first then build those out inside the larger queries. Let's save our query for athletes who have won medals as `medal_athlete_qry`. It's important that when we save it, we use `glue`/f-strings to make sure our string is expanded properly. We can then use `medal_athlete_qry` as a subquery to count the number of medals won by each country.\n\n::: {.panel-tabset}\n\n##### `R`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/SubSubquery.R_38579b07aa44b6c5ac5158f039bd36a4'}\n\n```{.r .cell-code}\nmedal_athlete_qry <- glue('\n  SELECT *\n  FROM ({athlete_bio_vw_qry})\n  WHERE athlete_id IN ({athlete_results_qry})')\ntbl_country_medal <- sdf_sql(sc, glue('\n  SELECT country, COUNT(country) AS medal_count\n  FROM ({medal_athlete_qry})\n  GROUP BY country\n  ORDER BY medal_count DESC'))\n\nhead(tbl_country_medal)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source: spark<?> [?? x 2]\n  country          medal_count\n  <chr>                  <dbl>\n1 \" United States\"        3116\n2 \" Soviet Union\"         1319\n3 \" Germany\"              1005\n4 \" Canada\"                886\n5 \" France\"                803\n6 \" Australia\"             797\n```\n:::\n:::\n\n\n##### `Python`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/SubSubquery.py_5b8893a5943da37df4d4c57642082448'}\n\n```{.python .cell-code}\nmedal_athlete_qry = f'''\n  SELECT *\n  FROM ({athlete_bio_vw_qry})\n  WHERE athlete_id IN ({athlete_results_qry})'''\ntbl_country_medal = sc.sql(f'''\n  SELECT country, COUNT(country) AS medal_count\n  FROM ({medal_athlete_qry})\n  GROUP BY country\n  ORDER BY medal_count DESC''')\n                   \ntbl_country_medal.show(6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------------+-----------+\n|       country|medal_count|\n+--------------+-----------+\n| United States|       3116|\n|  Soviet Union|       1319|\n|       Germany|       1005|\n|        Canada|        886|\n|        France|        803|\n|     Australia|        797|\n+--------------+-----------+\nonly showing top 6 rows\n```\n:::\n:::\n\n\n:::\n\n# Assigning Variable Tables to Spark Tables\n\nAt this point, you might decide you want to assign your new tables back to Spark, rather than only keeping them in your R or Python environment. This turns out to be a relatively simple operation and we name `tbl_athlete_bio_vw` to `athlete_bio_vw`. We can then check the tables available to us to make sure the operation succeeded, and then run a short SQL query to be extra sure it worked.\n\n::: {.panel-tabset}\n\n##### `R`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/SparkAgain.R_46dd8fa7510fba10bb04919ca7432a7f'}\n\n```{.r .cell-code}\ntbl_athlete_bio_vw <- copy_to(sc, tbl_athlete_bio_vw, 'athlete_bio_vw')\n\ndplyr::src_tbls(sc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"athlete_bio\"     \"athlete_bio_vw\"  \"athlete_results\" \"country\"        \n[5] \"games\"           \"medal_tally\"     \"results\"        \n```\n:::\n\n```{.r .cell-code}\nsdf_sql(sc, 'SELECT * FROM athlete_bio_vw LIMIT 6')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source: spark<?> [?? x 8]\n  athlete_id name                sex    born       height weight country count…¹\n       <int> <chr>               <chr>  <date>      <dbl>  <dbl> <chr>   <chr>  \n1      43737 Andrzej Socharski   Male   1947-08-31    173     72 \" Pola… POL    \n2      50147 Nathalie Wunderlich Female 1971-06-03    170     50 \" Swit… SUI    \n3       5085 Miha Lokar          Male   1935-09-10    182     76 \" Yugo… YUG    \n4     136329 Austin Hack         Male   1992-05-17    203    100 \" Unit… USA    \n5      38633 Tsuneo Ogasawara    Male   1942-07-30    181     80 \" Japa… JPN    \n6      77095 Fulgence Rwabu      Male   1947-11-23    165     51 \" Ugan… UGA    \n# … with abbreviated variable name ¹​country_noc\n```\n:::\n:::\n\n\n##### `Python`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/SparkAgain.py_ba7301a58f579331bdbae35f759f0c27'}\n\n```{.python .cell-code}\ntbl_athlete_bio_vw.createOrReplaceTempView('athlete_bio_vw')\n\nsc.sql(\"show tables\").show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+---------+---------------+-----------+\n|namespace|      tableName|isTemporary|\n+---------+---------------+-----------+\n|         |    athlete_bio|       true|\n|         | athlete_bio_vw|       true|\n|         |athlete_results|       true|\n|         |        country|       true|\n|         |          games|       true|\n|         |    medal_tally|       true|\n|         |        results|       true|\n+---------+---------------+-----------+\n```\n:::\n\n```{.python .cell-code}\nsc.sql('SELECT * FROM athlete_bio_vw LIMIT 6').show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+-------------------+------+----------+------+------+--------------+-----------+\n|athlete_id|               name|   sex|      born|height|weight|       country|country_noc|\n+----------+-------------------+------+----------+------+------+--------------+-----------+\n|     43737|  Andrzej Socharski|  Male|1947-08-31| 173.0|  72.0|        Poland|        POL|\n|     50147|Nathalie Wunderlich|Female|1971-06-03| 170.0|  50.0|   Switzerland|        SUI|\n|      5085|         Miha Lokar|  Male|1935-09-10| 182.0|  76.0|    Yugoslavia|        YUG|\n|    136329|        Austin Hack|  Male|1992-05-17| 203.0| 100.0| United States|        USA|\n|     38633|   Tsuneo Ogasawara|  Male|1942-07-30| 181.0|  80.0|         Japan|        JPN|\n|     77095|     Fulgence Rwabu|  Male|1947-11-23| 165.0|  51.0|        Uganda|        UGA|\n+----------+-------------------+------+----------+------+------+--------------+-----------+\n```\n:::\n:::\n\n\n:::\n\n# Dynamic Queries\n\nThe last piece that ties all of this together is using variables as different components of your data operations. A relatively common task could be to change dates on a monthly query to filter for the last thirty days. However, I don't think any Olympians have been born in the last thirty days, probably because the last Olympics was more than thirty days ago. Nevertheless, we can filter for athletes that were born in the last thirty years. In R, the `{lubridate}` makes the year subtraction a breeze while `python-dateutil` does all the heavy lifting in Python.\n\n::: {.panel-tabset}\n\n##### `R`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/Dynamic.R_72583a1c49a56262295135bed16f38e2'}\n\n```{.r .cell-code}\nlibrary(lubridate)\n\nthirtyYears <- ymd(Sys.Date()) - years(30)\n\nsdf_sql(sc, glue('\n  SELECT *\n  FROM athlete_bio_vw\n  WHERE born >= \"{thirtyYears}\"\n  LIMIT 6'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source: spark<?> [?? x 8]\n  athlete_id name               sex    born       height weight country  count…¹\n       <int> <chr>              <chr>  <date>      <dbl>  <dbl> <chr>    <chr>  \n1     136346 Pedro Pascual      Male   1996-03-15    185     70 \" Unite… USA    \n2     131521 Ana Dascăl         Female 2002-09-12    183     60 \" Roman… ROU    \n3     137091 Saskia Alusalu     Female 1994-04-14    175     64 \" Eston… EST    \n4     134220 Jason Osborne      Male   1994-03-20    178     72 \" Germa… GER    \n5     143729 C. A. Bhavani Devi Female 1993-08-27    168     58 \" India\" IND    \n6     138162 Oskar Svensson     Male   1995-09-07    190     86 \" Swede… SWE    \n# … with abbreviated variable name ¹​country_noc\n```\n:::\n:::\n\n\n##### `Python`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/Dynamic.py_100c8ade8792dcf8d22744650c60a4a0'}\n\n```{.python .cell-code}\nfrom datetime import date\nfrom dateutil.relativedelta import relativedelta\n\nthirtyYears = date.today() - relativedelta(years = 30)\nsc.sql(f'''\n  SELECT *\n  FROM athlete_bio_vw\n  WHERE born >= \"{thirtyYears}\"\n  LIMIT 6''').show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+------------------+------+----------+------+------+--------------+-----------+\n|athlete_id|              name|   sex|      born|height|weight|       country|country_noc|\n+----------+------------------+------+----------+------+------+--------------+-----------+\n|    136346|     Pedro Pascual|  Male|1996-03-15| 185.0|  70.0| United States|        USA|\n|    131521|        Ana Dascăl|Female|2002-09-12| 183.0|  60.0|       Romania|        ROU|\n|    137091|    Saskia Alusalu|Female|1994-04-14| 175.0|  64.0|       Estonia|        EST|\n|    134220|     Jason Osborne|  Male|1994-03-20| 178.0|  72.0|       Germany|        GER|\n|    143729|C. A. Bhavani Devi|Female|1993-08-27| 168.0|  58.0|         India|        IND|\n|    138162|    Oskar Svensson|  Male|1995-09-07| 190.0|  86.0|        Sweden|        SWE|\n+----------+------------------+------+----------+------+------+--------------+-----------+\n```\n:::\n:::\n\n\n:::\n\n# Disconnecting From Spark\n\nBefore we wrap up, let's disconnect from Spark real quick.\n\n::: {.panel-tabset}\n\n##### `R`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/Disconnect.R_6f03ffc2b5d6369e3c6e8afba6b6915b'}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::\n\n\n##### `Python`\n\n\n::: {.cell hash='2022-08-27-moving-local-sql-to-spark-the-easy-way_cache/html/Disconnect.py_a6d66ff19244ba7f10ebb5877816454b'}\n\n```{.python .cell-code}\nsc.stop()\n```\n:::\n\n\n:::\n\n# Wrapping Up\n\nAt the beginning of this post, I made a promise that moving your processes to the cloud doesn't have to be super painful. It probably still will be painful, but I'm hoping that it'll now be at most regular painful. We learned that we can either translate our queries to R or Python, run a SQL directly, or save the query as a string, and insert it into later queries where it will be run directly. We can then take our new tables and transfer them back over to Spark so we can reference the tables directly without any shenanigans involving `glue` or f-strings. Lastly, we can use those same `glue` and f-string tricks to dynamically change our queries based on R or Python variables.\n\n# Resources\n\nBoth R and Python have some really great resources out there to help you get started. I highly recommend starting with the documentation for `sparklyr` and `PySpark` as each have everything you need to know (besides the info in my post!) to get started.\n\n- R: [spark.rstudio.com](https://spark.rstudio.com)\n\n- Python: [PySpark](https://spark.apache.org/docs/latest/api/python/)\n\n----\n\nAll code in this article is available [here](https://github.com/guslipkin/guslipkin.github.io/blob/main/professional-blog/2022-08-27-moving-local-sql-to-spark-the-easy-way.qmd). If you want to see more from me, check out [my GitHub](https://github.com/guslipkin) or [guslipkin.github.io](https://guslipkin.github.io). If you want to hear from me, I'm also on Twitter [@guslipkin](https://twitter.com/GusLipkin).\n\n\n<center><em>Gus Lipkin is a Data Scientist, Business Analyst, and occasional bike mechanic</em></center>",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}