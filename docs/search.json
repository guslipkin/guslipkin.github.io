[
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact Me",
    "section": "",
    "text": "Name*\n\n\n\nCompany\n\n\n\nEmail*\n\n\n\nMessage*\n\n\n\n\n Sending this form will take you to an external thank you page\n\n\nSend"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Résumé",
    "section": "",
    "text": "Gus Lipkin\n\n\n\n\n\nFlorida Polytechnic University\n\n\nAug 2018 - May 2022\n\n\n\n– Bachelor of Science, Business Analytics with concentrations in Quantitative Economics & Econometrics and Intelligent Mobility  – Associate of Arts\n\n\nSelected Courses\n\n\n\n\n\n\nDatabase I & II\n\n\n\n\n\n\nStrategic Management\n\n\n\n\n\n\nSix Sigma\n\n\n\n\n\n\nProgramming I\n\n\n\n\n\n\nOperations and Supply Chain & Operations Research\n\n\n\n\n\n\nTime Series Modeling and Forecasting\n\n\n\n\n\n\nStatistics, Quantitative Methods, & Statistical Learning\n\n\n\n\n\n\nBenefit Cost Analysis & Economic Analysis\n\n\n\n\n\n\nSoftware and Programming\n\n\n\n\n\nR and RStudio\n\n\n\n\n\n\nSQL, Redis, Neo4J\n\n\n\n\n\n\nStata\n\n\n\n\n\n\nApache Spark\n\n\n\n\n\n\n\nData Analyst  – Publix Supermarkets\n\n\nMay 2022 - Present\n\n\n\n\nCreate and execute test plans to discover software problems and their causes using qTest and Jira\n\n\nInvestigate patterns in bugs reported using JQL and causal inference\n\n\nDocument and demonstrate software features to internal and external users in Confluence\n\n\n\n\nSoftware Quality Assurance Analyst  – AssistRx\n\n\nJan 2022 - May 2022\n\n\n\n\nCreate and execute test plans to discover software problems and their causes using qTest and Jira\n\n\nInvestigate patterns in bugs reported using JQL and causal inference\n\n\nDocument and demonstrate software features to internal and external users in Confluence\n\n\n\n\nData Science Project Team Lead  – Tallahassee Memorial Healthcare\n\n\nAug 2021 - May 2022\n\n\n\n\nCorrelate patient feedback to readmissions and patient experience using linear regression and decision trees\n\n\nCreate a training tool for nurses using significant results found in the analysis\n\n\nWork with capstone sponsor to define project timeline and goals and write a report on project findings\n\n\n\n\nResearch Intern  – iCompBio\n\n\nMay 2021 - Aug 2021\n\n\n\n\nGather and process time series data of geospatial climate variables and SARS-CoV-2 data\n\n\nAnalyze data in R, Excel, and ArcGIS to investigate relationships with time series analysis and linear regressions\n\n\n\n\nData Analyst and Assistant Project Manager  – Draken International\n\n\nDec 2020 - Sep 2021\n\n\n\n\nOversee a team of interns and supervise data migration efforts including cleaning source data before transfer\n\n\nUse text mining and sentiment analysis to correlate jet system failure reports and maintenance repairs\n\n\nCreate and run weekly parts availability analysis and reports using R and RStudio\n\n\n\n\n\nStudent Body Vice-President  – Florida Polytechnic University Student Government Association\n\n\nJan 2022 - Present\n\n\n\n\nAct as the Chief Services Officer of the Student Body\n\n\nAssist the Student Body President in the conduct of government\n\n\nStay in communication with all SGA departments and organizations\n\n\n\n\nDirector of Standards and Enforcement  – Florida Polytechnic University Student Government Association\n\n\nMay 2020 - Jan 2022\n\n\n\n\nManage the SGA SharePoint and CampusLabs\n\n\nTrack SGA and Registered Student Organization assets\n\n\nMaintain FLPolySGA.github.io"
  },
  {
    "objectID": "posts/2022-05-12-student-debt-and-retirement.html",
    "href": "posts/2022-05-12-student-debt-and-retirement.html",
    "title": "Does Student Debt Correlate with Retirement Funds?",
    "section": "",
    "text": "Pre-Intro\nI graduated with my BS in Business Analytics a few days ago. My mom came to visit for graduation and stayed for a few days. I wanted to show her some of what I learned at school through a hands on activity so we found a Tidy Tuesday dataset and did some exploring. There’s no particular rhyme or reason to this, but I wanted to share my passion for data with her and thought it would be cool to share with the rest of you as well. While I did provide some guidance, most of the exploration is what she thought might be interesting and wanted to explore.\n\n\n\n\n\n\nImportant\n\n\n\nFrom here on out, I wrote down my mom’s stream of consciousness thoughts on what we were working on. It’s not necessarily verbatim, but gets her thoughts across.\nItalics are my words\n\n\n\n\nIntro\nFirst we load our packages and data.\n\n\n\nDo you want to preview the data?\n\nhead(debt)\n\n   year     race loan_debt loan_debt_pct\n1: 2016    White 11108.410     0.3367511\n2: 2016    Black 14224.770     0.4183588\n3: 2016 Hispanic  7493.999     0.2189689\n4: 2013    White  8363.605     0.2845555\n5: 2013    Black 10302.660     0.4122773\n6: 2013 Hispanic  3177.410     0.1570289\n\nhead(retirement)\n\n   year     race retirement\n1: 1989    White  32649.430\n2: 1989    Black   5954.398\n3: 1989 Hispanic   7121.722\n4: 1992    White  36637.760\n5: 1992    Black   7798.197\n6: 1992 Hispanic   5248.894\n\n\nDo you want to see summary statistics?\nYeah. Can we do that?\n\nsummary(debt)\n\n      year          race             loan_debt       loan_debt_pct    \n Min.   :1989   Length:30          Min.   :  793.1   Min.   :0.09146  \n 1st Qu.:1995   Class :character   1st Qu.: 1406.8   1st Qu.:0.13887  \n Median :2002   Mode  :character   Median : 2992.6   Median :0.16049  \n Mean   :2002                      Mean   : 4119.4   Mean   :0.19388  \n 3rd Qu.:2010                      3rd Qu.: 5899.2   3rd Qu.:0.21823  \n Max.   :2016                      Max.   :14224.8   Max.   :0.41836  \n\nsummary(retirement)\n\n      year          race             retirement    \n Min.   :1989   Length:30          Min.   :  5249  \n 1st Qu.:1995   Class :character   1st Qu.: 15021  \n Median :2002   Mode  :character   Median : 21809  \n Mean   :2002                      Mean   : 41411  \n 3rd Qu.:2010                      3rd Qu.: 44936  \n Max.   :2016                      Max.   :157884  \n\n\n\n\nLooking at debt\nCan we see debt by race?\n\ndebt |>\n  ggplot() +\n  geom_boxplot(aes(x = race, y = loan_debt))\n\n\n\n\nIt’s been 40 years since I saw a box and whisker plot in college… But it’s interesting that Hispanics seem to have lower debt than Blacks and Whites who seem to be about equal.\nWhat about debt over time?\n\ndebt |>\n  ggplot() +\n  geom_point(aes(x = year, y = loan_debt, color = race))\n\n\n\n\nThe difference in debt between Blacks and Whites increases dramatically over time.\nDoes the share of families with debt change over time?\n\ndebt |>\n  ggplot() +\n  geom_point(aes(x = loan_debt_pct, y = loan_debt, color = race))\n\n\n\n\nIt appears that fewer Hispanics take on student loan debt than Blacks or Whites.\n\n\nLooking at retirement\nCan we copy/paste that code for retirement?\nYes. Yes we can!\n\nretirement |>\n  ggplot() +\n  geom_boxplot(aes(x = race, y = retirement))\n\n\n\n\nWhites seem to have much more solid retirement savings that Blacks or Hispanics. Even the lowest whisker in the Whites plot seems higher than the highest points in the Black or Hispanics plots.\n\nminMaxRetirement <- retirement |>\n  group_by(race) |>\n  summarise(min = min(retirement), max = max(retirement))\nminMaxRetirement\n\n# A tibble: 3 × 3\n  race        min     max\n  <chr>     <dbl>   <dbl>\n1 Black     5954.  29365.\n2 Hispanic  5249.  28581.\n3 White    32649. 157884.\n\nmax(minMaxRetirement$min) - min(minMaxRetirement$max)\n\n[1] 4068.31\n\n\nThis is terrible! The highest average Hispanic retirement is over $4000 less than the lowest average White retirement.\n\nretirement |>\n  ggplot() +\n  geom_point(aes(x = year, y = retirement, color = race))\n\n\n\n\nThis is not surprising. White retirement savings has increased faster than Black or Hispanic which has been relatively static.\n\n\nBringing it together\n\ndebt_retirement <- inner_join(debt, retirement, by = c(\"year\", \"race\"))\nhead(debt_retirement)\n\n   year     race loan_debt loan_debt_pct retirement\n1: 2016    White 11108.410     0.3367511  157884.20\n2: 2016    Black 14224.770     0.4183588   25211.85\n3: 2016 Hispanic  7493.999     0.2189689   28581.12\n4: 2013    White  8363.605     0.2845555  138557.50\n5: 2013    Black 10302.660     0.4122773   20440.14\n6: 2013 Hispanic  3177.410     0.1570289   10264.48\n\n\n\ndebt_retirement |>\n  ggplot() +\n  geom_point(aes(x = loan_debt, y = retirement, color = race))\n\n\n\n\nLoan debt doesn’t appear to have an effect on retirement for Blacks and Hispanics. It does have a large effect for Whites.\n\nlinear <- lm(retirement ~ loan_debt, data = debt_retirement[race == \"White\",])\nsummary(linear)\n\n\nCall:\nlm(formula = retirement ~ loan_debt, data = debt_retirement[race == \n    \"White\", ])\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13520 -10442  -6777  12753  22574 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 32652.056   8114.951   4.024  0.00382 ** \nloan_debt      12.284      1.419   8.657 2.46e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14480 on 8 degrees of freedom\nMultiple R-squared:  0.9036,    Adjusted R-squared:  0.8915 \nF-statistic: 74.95 on 1 and 8 DF,  p-value: 2.463e-05\n\n\nLoan debt doesn’t have the effect we thought for Whites, and seems to have minimal effect for Blacks and Hispanics.\n\nlinear <- lm(retirement ~ ., data = debt_retirement)\nsummary(linear)\n\n\nCall:\nlm(formula = retirement ~ ., data = debt_retirement)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-33124 -14811   5102  11353  38578 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -3.458e+06  1.744e+06  -1.983   0.0590 .  \nyear           1.733e+03  8.733e+02   1.985   0.0587 .  \nraceHispanic   7.743e+02  1.443e+04   0.054   0.9577    \nraceWhite      7.357e+04  1.156e+04   6.366 1.39e-06 ***\nloan_debt      6.832e-01  5.213e+00   0.131   0.8968    \nloan_debt_pct  6.545e+03  2.032e+05   0.032   0.9746    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19760 on 24 degrees of freedom\nMultiple R-squared:  0.8291,    Adjusted R-squared:  0.7935 \nF-statistic: 23.29 on 5 and 24 DF,  p-value: 1.738e-08\n\n\nI wonder if people with loan debt haven’t finished school?\nStudent loan debt could also be affected by family size which we don’t have data for.\n\n\nWhat if we account for wealth too?\n\nwealth <- \n  fread('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-02-09/race_wealth.csv') |>\n  rename(wealth_type = type)\nhead(wealth)\n\n   wealth_type year      race wealth_family\n1:     Average 1963 Non-White      19503.84\n2:     Average 1963     White     140632.66\n3:     Average 1963     Black            NA\n4:     Average 1963  Hispanic            NA\n5:     Average 1983 Non-White      73233.62\n6:     Average 1983     White     324057.60\n\n\n\nsummary(wealth)\n\n wealth_type             year          race           wealth_family   \n Length:96          Min.   :1963   Length:96          Min.   :  2467  \n Class :character   1st Qu.:1991   Class :character   1st Qu.: 19559  \n Mode  :character   Median :2000   Mode  :character   Median : 97209  \n                    Mean   :1998                      Mean   :158020  \n                    3rd Qu.:2008                      3rd Qu.:156895  \n                    Max.   :2016                      Max.   :919336  \n                                                      NA's   :24      \n\n\n\nwealth |>\n  ggplot() +\n  geom_boxplot(aes(x = race, y = wealth_family))\n\nWarning: Removed 24 rows containing non-finite values (stat_boxplot).\n\n\n\n\n\nOh. That’s interesting. I think family wealth includes all assets and debts, liquid or not. That’s not a great indication of actual wealth.\nIt looks like there’s some data missing so lets filter for years greater than or equal to 1985 and for the median, rather than average, income.\n\nwealth |>\n  filter(year >= 1985, wealth_type == \"Median\") |>\n  ggplot() +\n  geom_point(aes(x = year, y = wealth_family, color = race))\n\nWarning: Removed 10 rows containing missing values (geom_point).\n\n\n\n\n\nIt looks like there isn’t any data for Non-Whites. The median has increased for Whites, but dropped in 2008, while for others it has remained the same over time.\n\n\nAdd wealth into the mix\n\ndf <- left_join(debt_retirement, wealth, by = c(\"race\", \"year\"))\nhead(df)\n\n   year     race loan_debt loan_debt_pct retirement wealth_type wealth_family\n1: 2016    White 11108.410     0.3367511  157884.20     Average      919336.1\n2: 2016    White 11108.410     0.3367511  157884.20      Median      171000.0\n3: 2016    Black 14224.770     0.4183588   25211.85     Average      139523.1\n4: 2016    Black 14224.770     0.4183588   25211.85      Median       17409.0\n5: 2016 Hispanic  7493.999     0.2189689   28581.12     Average      191727.3\n6: 2016 Hispanic  7493.999     0.2189689   28581.12      Median       20920.0\n\n\nIt’s kinda confusing with wealth_type, let’s change it a bit\n\ndf <- df |>\n  dcast(year + race + loan_debt + loan_debt_pct + retirement ~ wealth_type, \n        value.var = \"wealth_family\") |>\n  rename(average_wealth = Average, median_wealth = Median)\nhead(df)\n\n   year     race loan_debt loan_debt_pct retirement average_wealth\n1: 1989    Black 1160.5680     0.1788198   5954.398       78092.20\n2: 1989 Hispanic  897.5826     0.1272523   7121.722       84397.75\n3: 1989    White 1100.4070     0.1047123  32649.430      424082.40\n4: 1992    Black  927.4824     0.1382771   7798.197       80779.48\n5: 1992 Hispanic  793.0610     0.0914574   5248.894       90751.79\n6: 1992    White 1321.3030     0.1433340  36637.760      373825.90\n   median_wealth\n1:      8023.198\n2:      9329.301\n3:    134677.800\n4:     16602.980\n5:     11387.300\n6:    116891.700\n\n\n“Can we scatter this one?”\nYes. Yes we can.\n\ndf |>\n  ggplot() +\n  geom_point(aes(x = loan_debt, y = average_wealth, color = race))\n\n\n\n\nHispanics only seem to be doing better with having less loan debt.\nCan we predict if increasing loan debt is correlated to average wealth?\n\nsummary(lm(average_wealth ~ loan_debt + race, data = df))\n\n\nCall:\nlm(formula = average_wealth ~ loan_debt + race, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-186449  -33795   17625   48869  180878 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.341e+04  3.989e+04   0.336  0.73941    \nloan_debt    1.821e+01  5.133e+00   3.547  0.00151 ** \nraceHispanic 7.419e+04  4.435e+04   1.673  0.10639    \nraceWhite    5.228e+05  4.219e+04  12.391 2.05e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 94210 on 26 degrees of freedom\nMultiple R-squared:  0.8861,    Adjusted R-squared:  0.873 \nF-statistic: 67.45 on 3 and 26 DF,  p-value: 2.139e-12\n\n\nThis makes sense and tracks with what we saw in the other regressions.\n\n\nBringing it back to retirement\n\nsummary(lm(retirement ~ ., data = df))\n\n\nCall:\nlm(formula = retirement ~ ., data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-11882  -4546     -3   2733  16560 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -6.352e+05  6.836e+05  -0.929  0.36291    \nyear            3.097e+02  3.432e+02   0.902  0.37666    \nraceHispanic   -4.747e+03  5.986e+03  -0.793  0.43628    \nraceWhite       6.748e+03  1.404e+04   0.481  0.63545    \nloan_debt      -2.796e+00  1.906e+00  -1.467  0.15656    \nloan_debt_pct   1.130e+05  7.727e+04   1.463  0.15763    \naverage_wealth  2.716e-01  2.798e-02   9.709 2.06e-09 ***\nmedian_wealth  -5.232e-01  1.756e-01  -2.980  0.00691 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7119 on 22 degrees of freedom\nMultiple R-squared:  0.9797,    Adjusted R-squared:  0.9732 \nF-statistic: 151.4 on 7 and 22 DF,  p-value: < 2.2e-16\n\n\nI guess this makes sense. If you have money, you have money for retirement.\nLet me show you that R package I made that I always talk about.\n\ndewey::regsearch(df, dependent = \"retirement\", \n                 independent = colnames(df)[c(1:4, 6:7)], maxvar = 6, \n                 family = \"gaussian\") |>\n  head(12) |>\n  kbl() |>\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \n                                      \"condensed\", \"responsive\"),\n                font_size = 14) |>\n  scroll_box(height = \"250px\")\n\nWarning in dewey::regsearch(df, dependent = \"retirement\", independent =\ncolnames(df)[c(1:4, : Missing 'interactions' argument. Defaulting to FALSE.\n\n\nWarning in dewey::regsearch(df, dependent = \"retirement\", independent =\ncolnames(df)[c(1:4, : Missing 'multi' argument. Defaulting to FALSE.\n\n\n[1] \"Assembling regresions...\"\n[1] \"Creating 63 formulas. Please be patient, this may take a while.\"\n[1] \"Creating regressions...\"\n[1] \"Running 63 regressions. Please be patient, this may take a while.\"\n[1] \"Running regressions...\"\n\n\n\n\n \n  \n    formula \n    aic \n    bic \n    rSquare \n    warn \n    xIntercept \n    year \n    race \n    loan_debt \n    loan_debt_pct \n    average_wealth \n    median_wealth \n    raceHispanic \n    raceWhite \n  \n \n\n  \n    retirement ~ + average_wealth \n    650.2739 \n    654.4775 \n    0.93199 \n    NA \n    0.1895266 \n    NA \n    NA \n    NA \n    NA \n    0 \n    NA \n    NA \n    NA \n  \n  \n    retirement ~ + median_wealth \n    686.7959 \n    690.9995 \n    0.77025 \n    NA \n    0.3351785 \n    NA \n    NA \n    NA \n    NA \n    NA \n    0.0000000 \n    NA \n    NA \n  \n  \n    retirement ~ + average_wealth + median_wealth \n    629.9331 \n    635.5379 \n    0.96771 \n    NA \n    0.0036909 \n    NA \n    NA \n    NA \n    NA \n    0 \n    0.0000088 \n    NA \n    NA \n  \n  \n    retirement ~ + median_wealth + year \n    670.4355 \n    676.0403 \n    0.87542 \n    NA \n    0.0000571 \n    0.0000559 \n    NA \n    NA \n    NA \n    NA \n    0.0000000 \n    NA \n    NA \n  \n  \n    retirement ~ + loan_debt + median_wealth \n    673.2736 \n    678.8784 \n    0.86306 \n    NA \n    0.1440991 \n    NA \n    NA \n    0.0002116 \n    NA \n    NA \n    0.0000000 \n    NA \n    NA \n  \n  \n    retirement ~ + average_wealth + loan_debt_pct \n    638.7578 \n    644.3626 \n    0.95666 \n    NA \n    0.0003023 \n    NA \n    NA \n    NA \n    0.0005465 \n    0 \n    NA \n    NA \n    NA \n  \n  \n    retirement ~ + average_wealth + loan_debt \n    639.8237 \n    645.4284 \n    0.95509 \n    NA \n    0.0030143 \n    NA \n    NA \n    0.0009078 \n    NA \n    0 \n    NA \n    NA \n    NA \n  \n  \n    retirement ~ + average_wealth + year \n    640.3430 \n    645.9478 \n    0.95431 \n    NA \n    0.0011340 \n    0.0011635 \n    NA \n    NA \n    NA \n    0 \n    NA \n    NA \n    NA \n  \n  \n    retirement ~ + loan_debt_pct + median_wealth \n    676.7298 \n    682.3345 \n    0.84633 \n    NA \n    0.0215718 \n    NA \n    NA \n    NA \n    0.0010906 \n    NA \n    0.0000000 \n    NA \n    NA \n  \n  \n    retirement ~ + average_wealth + loan_debt_pct + median_wealth \n    624.0222 \n    631.0282 \n    0.97519 \n    NA \n    0.0001792 \n    NA \n    NA \n    NA \n    0.0094886 \n    0 \n    0.0001608 \n    NA \n    NA \n  \n  \n    retirement ~ + average_wealth + loan_debt + loan_debt_pct + median_wealth \n    621.5912 \n    629.9984 \n    0.97860 \n    NA \n    0.0002454 \n    NA \n    NA \n    0.0570795 \n    0.0087417 \n    0 \n    0.0000324 \n    NA \n    NA \n  \n  \n    retirement ~ + average_wealth + loan_debt + median_wealth \n    628.0037 \n    635.0097 \n    0.97167 \n    NA \n    0.0007230 \n    NA \n    NA \n    0.0675575 \n    NA \n    0 \n    0.0006058 \n    NA \n    NA \n  \n\n\n\n\n\nTry choosing a model and defend your choice\nI would choose retirement ~ + average_wealth + loan_debt + loan_debt_pct + median_wealth because it has the highest \\(R^2\\) value.\nThat’s not quite right. retirement ~ + average_wealth + median_wealth is much better because 96% of the model is explained by those two variables so adding loan_debt_pct and loan_debt does not give a lot of benefit.\nWhat if we considered interaction terms?\n\ndewey::regsearch(df, dependent = \"retirement\", \n                 independent = colnames(df)[c(1:4, 6:7)], maxvar = 6, \n                 family = \"gaussian\", interactions = TRUE) |>\n  head(12) |>\n  kbl() |>\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \n                                      \"condensed\", \"responsive\"),\n                font_size = 14) |>\n  scroll_box(height = \"250px\")\n\nWarning in dewey::regsearch(df, dependent = \"retirement\", independent =\ncolnames(df)[c(1:4, : Missing 'multi' argument. Defaulting to FALSE.\n\n\n[1] \"Gathering variables...\"\n[1] \"WARNING: Using interaction terms without multithreading may take a very long time\"\n[1] \"Assembling regresions...\"\n[1] \"Creating 82159 formulas. Please be patient, this may take a while.\"\n[1] \"Creating regressions...\"\n[1] \"Running 14828 regressions. Please be patient, this may take a while.\"\n[1] \"Running regressions...\"\n\n\n\n\n \n  \n    formula \n    aic \n    bic \n    rSquare \n    warn \n    xIntercept \n    year \n    race \n    loan_debt \n    loan_debt_pct \n    average_wealth \n    median_wealth \n    average_wealth.loan_debt \n    average_wealth.loan_debt_pct \n    average_wealth.median_wealth \n    raceHispanic \n    raceWhite \n    average_wealth.raceHispanic \n    average_wealth.raceWhite \n    average_wealth.year \n    loan_debt.loan_debt_pct \n    loan_debt.median_wealth \n    loan_debt.raceHispanic \n    loan_debt.raceWhite \n    loan_debt.year \n    loan_debt_pct.median_wealth \n    loan_debt_pct.raceHispanic \n    loan_debt_pct.raceWhite \n    loan_debt_pct.year \n    median_wealth.raceHispanic \n    median_wealth.raceWhite \n    median_wealth.year \n    raceHispanic.year \n    raceWhite.year \n    loan_debt_pct.loan_debt \n    median_wealth.loan_debt \n    median_wealth.loan_debt_pct \n    raceHispanic.loan_debt \n    raceWhite.loan_debt \n    raceHispanic.loan_debt_pct \n    raceWhite.loan_debt_pct \n    raceHispanic.median_wealth \n    raceWhite.median_wealth \n    year.loan_debt \n    year.loan_debt_pct \n    year.median_wealth \n    year.raceHispanic \n    year.raceWhite \n  \n \n\n  \n    retirement ~ + average_wealth \n    650.2739 \n    654.4775 \n    0.93199 \n    NA \n    0.1895266 \n    NA \n    NA \n    NA \n    NA \n    0.0000000 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n  \n  \n    retirement ~ + median_wealth \n    686.7959 \n    690.9995 \n    0.77025 \n    NA \n    0.3351785 \n    NA \n    NA \n    NA \n    NA \n    NA \n    0.0000000 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n  \n  \n    retirement ~ + average_wealth + median_wealth \n    629.9331 \n    635.5379 \n    0.96771 \n    NA \n    0.0036909 \n    NA \n    NA \n    NA \n    NA \n    0.0000000 \n    0.0000088 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n  \n  \n    retirement ~ + median_wealth + year \n    670.4355 \n    676.0403 \n    0.87542 \n    NA \n    0.0000571 \n    0.0000559 \n    NA \n    NA \n    NA \n    NA \n    0.0000000 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n  \n  \n    retirement ~ + loan_debt + median_wealth \n    673.2736 \n    678.8784 \n    0.86306 \n    NA \n    0.1440991 \n    NA \n    NA \n    0.0002116 \n    NA \n    NA \n    0.0000000 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n  \n  \n    retirement ~ + average_wealth + loan_debt_pct \n    638.7578 \n    644.3626 \n    0.95666 \n    NA \n    0.0003023 \n    NA \n    NA \n    NA \n    0.0005465 \n    0.0000000 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n  \n  \n    retirement ~ + average_wealth + loan_debt \n    639.8237 \n    645.4284 \n    0.95509 \n    NA \n    0.0030143 \n    NA \n    NA \n    0.0009078 \n    NA \n    0.0000000 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n  \n  \n    retirement ~ + average_wealth + year \n    640.3430 \n    645.9478 \n    0.95431 \n    NA \n    0.0011340 \n    0.0011635 \n    NA \n    NA \n    NA \n    0.0000000 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n  \n  \n    retirement ~ + loan_debt_pct + median_wealth \n    676.7298 \n    682.3345 \n    0.84633 \n    NA \n    0.0215718 \n    NA \n    NA \n    NA \n    0.0010906 \n    NA \n    0.0000000 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n  \n  \n    retirement ~ + average_wealth*loan_debt_pct + average_wealth*median_wealth + average_wealth*race + average_wealth*year \n    576.6673 \n    594.8828 \n    0.99700 \n    NA \n    0.0004973 \n    0.0004812 \n    NA \n    NA \n    0.0004864 \n    0.0000061 \n    0.0008194 \n    NA \n    0.0004452 \n    2.50e-06 \n    0.0020289 \n    0.0045869 \n    0.0087832 \n    0.0120152 \n    5.8e-06 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n  \n  \n    retirement ~ + average_wealth + loan_debt_pct + median_wealth \n    624.0222 \n    631.0282 \n    0.97519 \n    NA \n    0.0001792 \n    NA \n    NA \n    NA \n    0.0094886 \n    0.0000000 \n    0.0001608 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n  \n  \n    retirement ~ + average_wealth*median_wealth + median_wealth*year + loan_debt_pct \n    590.8267 \n    602.0363 \n    0.99328 \n    NA \n    0.0081282 \n    0.0076968 \n    NA \n    NA \n    0.0126352 \n    0.0004052 \n    0.0000001 \n    NA \n    NA \n    2.14e-05 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    1e-07 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n  \n\n\n\n\n\nThis is pretty cool. Average wealth by itself is pretty good, but when it interacts with loan_debt_pct, median_wealth, race, and year, it seems to produce a really great model.\nOf course, we still want to be careful about overfitting.\n\n\nParting Thoughts\nRace appears to have a huge impact on everything and increasing educational debt increases retirement, but only for Whites. I’d be curious to see the data behind the loans to learn why increasing educational debt does not always correlate with increased wealth.\n\n\nPost-Conclusion\nIt’s Gus again. I had a really fun time with this. I like to think my mom learned a lot and I’m hoping you did too."
  },
  {
    "objectID": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html",
    "href": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html",
    "title": "Pivoting Data with tidyverse and data.table in R",
    "section": "",
    "text": "Pivoting data can be a little scary sometimes, even if you know exactly what you want to do. But it doesn’t need to be. I’m hoping that by the end of this post, you’ll feel like the pivot pro you always knew you could be.\n\n\n\n\n\n\n\n(a) A giraffe (longer)\n\n\n\n\n\n\n\n(b) A manta ray (wider)\n\n\n\n\nFigure 1: Longer and wider animals\n\n\n\n\nAs usual, the first thing we want to do is load our libraries. We’ll be using pivot_wider and pivot_longer from tidyverse as well as the pipe operator, %>% from magrittr which is loaded through tidyverse. We’re also using dcast and melt from data.table as they are twins to pivot_wider and pivot_longer, respectively. While they are not identical, they perform the same functions are operate somewhat similarly.\n\nlibrary(tidyverse)\nlibrary(data.table)\n\nNext we want to create our dummy data. I’ve been working with theme park data a lot recently, so I modeled the dummy data after that. The data description is as follows:\n\n\n\n\n\n\n\n\n\nvariable\ntype\ndescription\n\n\n\n\nrideName\ncharacter\nThe full name of the ride\n\n\nshortName\ncharacter\nA shortened version of the name of the ride\n\n\ntime\nITime (from data.table)\nThe time the data was recorded (every 5 minutes from 8am to 5pm)\n\n\nattendance\ndouble\nThe theme park attendance during the time recorded\n\n\nestimatedWait\ndouble\nThe estimated wait time in minutes\n\n\nactualWait\ndouble\nThe actual wait time in minutes\n\n\n\n\n\nset.seed(2022)\nrideName <- rep(c(\"The Amulet from Below\", \"The Strong Key Mystery\",\n                  \"The Punishment of Bane\", \"The Mage Beyond the Grave\",\n                  \"The Fury from Beyond\"), 108)\nshortName <- rep(c(\"amulet\", \"key\", \"punishment\", \"mage\", \"fury\"), 108)\ntime <- as.ITime(rep(seq(8*3600, 17*3600-1, by = 60*5), times = 1, each = 5))\nattendance <- rep(round(rnorm(108, 5000, 100)), times = 1, each = 5)\nestimatedWait <- round(rnorm(length(rideName), 50, 10))\nactualWait <- round(rnorm(length(rideName), estimatedWait, 5))\n\ndt <- data.table(rideName, shortName, time, attendance, estimatedWait, actualWait)\n\nOnce created, the data looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\nrideName\nshortName\ntime\nattendance\nestimatedWait\nactualWait\n\n\n\n\nThe Amulet from Below\namulet\n08:00:00\n5090\n53\n54\n\n\nThe Strong Key Mystery\nkey\n08:00:00\n5090\n62\n64\n\n\nThe Punishment of Bane\npunishment\n08:00:00\n5090\n56\n55\n\n\nThe Mage Beyond the Grave\nmage\n08:00:00\n5090\n40\n33\n\n\nThe Fury from Beyond\nfury\n08:00:00\n5090\n30\n19\n\n\nThe Amulet from Below\namulet\n08:05:00\n4883\n45\n48"
  },
  {
    "objectID": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#pivot_wider-and-dcast",
    "href": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#pivot_wider-and-dcast",
    "title": "Pivoting Data with tidyverse and data.table in R",
    "section": "pivot_wider and dcast",
    "text": "pivot_wider and dcast\npivot_wider and dcast take data and reshapes it so that there are more columns and fewer rows than the input data. It allows you to specify a column as a unique identifier and use the values in one or more columns as new column names. The last column is the values that you want placed in the proper intersection between the identifier column value and each new column value.\n\n\n\n\n\n\n\n(a) The original data\n\n\n\n\n\n\n\n(b) The wider data\n\n\n\n\nFigure 2: Wider data expands columns and decreases length\n\n\nIn the example above, I’ve pivoted the actualWait using the time and shortName columns. We can see that the first five rows of values in actualWait have become the first row of values in the wider data. The last row with The Amulet from Below at 8:05 becomes the value for amulet in the second row of the wider data where the time is 8:05.\nWhile pivot_wider and dcast perform the same function, they behave a little bit differently. pivot_wider uses three main arguments id_cols, names_from, and values_from. dcast, on the other hand, uses a formula argument in place of both id_cols and names_from and value.vars in place of values_from. The second big difference is that column names in pivot_wider don’t have to be in quotes. They can be, but they don’t have to be. On the other hand, only the formula in dcast can go without quotes, and in that case they must not have quotes while value.var must have quotes."
  },
  {
    "objectID": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#pivot_longer-and-melt",
    "href": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#pivot_longer-and-melt",
    "title": "Pivoting Data with tidyverse and data.table in R",
    "section": "pivot_longer and melt",
    "text": "pivot_longer and melt\npivot_longer and melt take data and reshape it so that there are more rows and fewer columns than the input data. It allows you to specify specify columns that you want aggregated and the new column names for what was the column names and the values column.\n\n\n\n\n\n\n\n(a) The original data\n\n\n\n\n\n\n\n(b) The longer data\n\n\n\n\nFigure 3: Longer data combines columns and decreases width\n\n\nIn the example above, the estimatedWait and actualWait columns are consolidated into the waitType column where each row specifies if the corresponding waitTime column is an estimated or actual wait time.\nLike the previous two, pivot_longer and melt have some differences, although these are smaller. pivot_longer uses three primary arguments, cols, names_to, and values_to which correspond to measure.vars, variable.name, and value.name, respectively, from melt. Again, the data.table version, melt requires quotes around each argument value while pivot_longer only requires quotes around names_to and values_to because cols is acting as a select statement."
  },
  {
    "objectID": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#wider-with-one-value-column",
    "href": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#wider-with-one-value-column",
    "title": "Pivoting Data with tidyverse and data.table in R",
    "section": "Wider with One Value Column",
    "text": "Wider with One Value Column\nIn this example, we want to know how long the actualWait is for each ride throughout the day with each recording time as a row, and each ride as a column.\nIn the pivot_wider, we want our new column names_from to be the shortName and our values_from actualWait. The tricky bit here is the id_cols argument. id_cols are columns whose values should uniquely identify each row in the data. We need to use it because the rideName and shortName columns are perfectly correlated, that is, for each row the value of both columns will always be the same. Any column that is not in the names_from or values_from argument will be part of the default id_cols argument. We can use the argument two ways; we can use a numeric vector to select the columns that we want to be used as identifiers, or we can specify columns that we do not want used as a character or column name vector. In the example below, I’ve included both methods. For the rest of the post, I’ll be using the second method.\nAs mentioned previously, the biggest difference for dcast is that it uses a formula instead of the id_cols and names_from arguments. The left hand side of the formula is the columns that you want to stay as columns to be used as the unique identifier for each row. The right hand side of the formula is the column whose values will be used as the new column names. Lastly, value.var defines the column with the desired values.\n\n# pivot wider with one value column\ndt %>% pivot_wider(id_cols = 2:3, \n                   names_from = shortName, \n                   values_from = actualWait)\ndt %>% pivot_wider(id_cols = !c(rideName, attendance, estimatedWait), \n                   names_from = shortName, \n                   values_from = actualWait)\n\ndt %>% dcast(time ~ shortName, \n             value.var = \"actualWait\")\n\n\n\n\n\ntime\namulet\nkey\npunishment\nmage\nfury\n\n\n\n\n08:00:00\n54\n64\n55\n33\n19\n\n\n08:05:00\n48\n54\n40\n56\n55\n\n\n08:10:00\n59\n44\n50\n52\n35\n\n\n08:15:00\n47\n71\n46\n41\n59\n\n\n08:20:00\n26\n68\n64\n47\n48\n\n\n08:25:00\n35\n32\n54\n54\n57"
  },
  {
    "objectID": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#wider-with-two-or-more-value-columns",
    "href": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#wider-with-two-or-more-value-columns",
    "title": "Pivoting Data with tidyverse and data.table in R",
    "section": "Wider with Two or More Value Columns",
    "text": "Wider with Two or More Value Columns\nWider data with more than one value column is very similar to one value column. The only changes you need to make in pivot_wider are changing your id_cols argument if using the column name method and adding the new column to the values_from argument. The only change needed in dcast is adding to the value.var argument. One important thing to note is that the column names will now be a combination of the names_from/right hand side argument and the values_from/value.var argument. The column name order in the value* argument determines the column order in the resulting data.\n\n# pivot wider with two value columns\ndt %>% pivot_wider(id_cols = !c(rideName, attendance), \n                   names_from = shortName, \n                   values_from = c(estimatedWait, actualWait))\n\ndt %>% dcast(time ~ shortName, \n             value.var = c(\"estimatedWait\", \"actualWait\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime\nestimatedWait_amulet\nestimatedWait_key\nestimatedWait_punishment\nestimatedWait_mage\nestimatedWait_fury\nactualWait_amulet\nactualWait_key\nactualWait_punishment\nactualWait_mage\nactualWait_fury\n\n\n\n\n08:00:00\n53\n62\n56\n40\n30\n54\n64\n55\n33\n19\n\n\n08:05:00\n45\n50\n36\n52\n52\n48\n54\n40\n56\n55\n\n\n08:10:00\n59\n41\n47\n57\n37\n59\n44\n50\n52\n35\n\n\n08:15:00\n45\n61\n47\n32\n62\n47\n71\n46\n41\n59\n\n\n08:20:00\n33\n69\n61\n50\n50\n26\n68\n64\n47\n48\n\n\n08:25:00\n31\n34\n47\n55\n60\n35\n32\n54\n54\n57"
  },
  {
    "objectID": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#pivot-wider-two-or-more-id-columns",
    "href": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#pivot-wider-two-or-more-id-columns",
    "title": "Pivoting Data with tidyverse and data.table in R",
    "section": "Pivot Wider Two or More ID Columns",
    "text": "Pivot Wider Two or More ID Columns\nAgain, this is pretty similar to a “normal” wider operation. With pivot_wider you expand your id_cols to include the new columns, or, in my case, drop the attendance column so that it is now included. With dcast, you simply add the new column to the left hand side of the formula.\n\n# pivot wider with two id columns\ndt %>% pivot_wider(id_cols = !c(rideName, estimatedWait),\n                   names_from = shortName,\n                   values_from = actualWait)\n\ndt %>% dcast(time + attendance ~ shortName,\n             value.var = \"actualWait\")\n\n\n\n\n\ntime\nattendance\namulet\nkey\npunishment\nmage\nfury\n\n\n\n\n08:00:00\n5090\n54\n64\n55\n33\n19\n\n\n08:05:00\n4883\n48\n54\n40\n56\n55\n\n\n08:10:00\n4910\n59\n44\n50\n52\n35\n\n\n08:15:00\n4856\n47\n71\n46\n41\n59\n\n\n08:20:00\n4967\n26\n68\n64\n47\n48\n\n\n08:25:00\n4710\n35\n32\n54\n54\n57"
  },
  {
    "objectID": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#pivot-wider-with-two-or-more-name-columns",
    "href": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#pivot-wider-with-two-or-more-name-columns",
    "title": "Pivoting Data with tidyverse and data.table in R",
    "section": "Pivot Wider with Two or More Name Columns",
    "text": "Pivot Wider with Two or More Name Columns\nWe want to take our new longer data and change it to wider data where we want to know how long the actualWait and estimatedWait are for each ride throughout the day with each recording time as a row, and each ride as a column and waitType and shortName.\nFor pivot_wider, we want our column names to use both the waitType and shortName and our values from waitTime. id_cols is then the remaining columns that we don’t want. With dcast, we want to add waitType and shortName to the right hand side of the formula so they are used as the column names, and time to the left hand side for the row identifiers. For both, values_from and value.var are our waitTime column.\nLike when using multiple values in value_from/value.var, the new column names are all combinations of values in names_from or right hand side. The combinations will be given in the order the column names are specified.\n\n# pivot wider with more than one right hand column\ndtLonger %>% pivot_wider(id_cols = !c(rideName, attendance), \n                         names_from = c(waitType, shortName), \n                         values_from = waitTime)\n\ndtLonger %>% dcast(time ~ waitType + shortName, \n                   value.var = \"waitTime\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime\nestimatedWait_amulet\nestimatedWait_key\nestimatedWait_punishment\nestimatedWait_mage\nestimatedWait_fury\nactualWait_amulet\nactualWait_key\nactualWait_punishment\nactualWait_mage\nactualWait_fury\n\n\n\n\n08:00:00\n53\n62\n56\n40\n30\n54\n64\n55\n33\n19\n\n\n08:05:00\n45\n50\n36\n52\n52\n48\n54\n40\n56\n55\n\n\n08:10:00\n59\n41\n47\n57\n37\n59\n44\n50\n52\n35\n\n\n08:15:00\n45\n61\n47\n32\n62\n47\n71\n46\n41\n59\n\n\n08:20:00\n33\n69\n61\n50\n50\n26\n68\n64\n47\n48\n\n\n08:25:00\n31\n34\n47\n55\n60\n35\n32\n54\n54\n57"
  },
  {
    "objectID": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#pivot-wider-with-mutiple-id-and-name-columns",
    "href": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#pivot-wider-with-mutiple-id-and-name-columns",
    "title": "Pivoting Data with tidyverse and data.table in R",
    "section": "Pivot Wider with Mutiple ID and Name Columns",
    "text": "Pivot Wider with Mutiple ID and Name Columns\nIf you’ve been following along so far, the code below should make sense. We want to use time and attendance as the row identifiers and the estimatedWait and actualWait as the values for each ride.\n\n# pivot wider with more than one left and right hand column\ndtLonger %>% pivot_wider(id_cols = !rideName, \n                         names_from = c(waitType, shortName), \n                         values_from = waitTime)\n\ndtLonger %>% dcast(time + attendance ~ waitType + shortName, \n                   value.var = \"waitTime\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime\nattendance\nestimatedWait_amulet\nestimatedWait_key\nestimatedWait_punishment\nestimatedWait_mage\nestimatedWait_fury\nactualWait_amulet\nactualWait_key\nactualWait_punishment\nactualWait_mage\nactualWait_fury\n\n\n\n\n08:00:00\n5090\n53\n62\n56\n40\n30\n54\n64\n55\n33\n19\n\n\n08:05:00\n4883\n45\n50\n36\n52\n52\n48\n54\n40\n56\n55\n\n\n08:10:00\n4910\n59\n41\n47\n57\n37\n59\n44\n50\n52\n35\n\n\n08:15:00\n4856\n45\n61\n47\n32\n62\n47\n71\n46\n41\n59\n\n\n08:20:00\n4967\n33\n69\n61\n50\n50\n26\n68\n64\n47\n48\n\n\n08:25:00\n4710\n31\n34\n47\n55\n60\n35\n32\n54\n54\n57"
  },
  {
    "objectID": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#a-quick-benchmark",
    "href": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#a-quick-benchmark",
    "title": "Pivoting Data with tidyverse and data.table in R",
    "section": "A quick benchmark",
    "text": "A quick benchmark\n\nrbenchmark::benchmark(\n  \"pivot_wider\" = {\n    tmp <- dt %>% pivot_wider(id_cols = !c(rideName, attendance, estimatedWait), \n                              names_from = shortName, \n                              values_from = actualWait)\n  },\n  \"dcast\" = {\n    tmp <- dt %>% dcast(time ~ shortName, \n                        value.var = \"actualWait\")\n  }, order = \"user.self\")\n\nrbenchmark::benchmark(\n  \"pivot_longer\" = {\n    tmp <- dt %>% pivot_longer(cols = c(estimatedWait, actualWait), \n                               names_to = \"waitType\", \n                               values_to = \"waitTime\")\n  },\n  \"melt\" = {\n    tmp <- dt %>% melt(measure.vars = c(\"estimatedWait\", \"actualWait\"),\n                       variable.name = \"waitType\",\n                       value.name = \"waitTime\")\n  }, order = \"user.self\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest\nreplications\nelapsed\nrelative\nuser.self\nsys.self\nuser.child\nsys.child\n\n\n\n\ndcast\n100\n0.060\n1.00\n0.059\n0.001\n0\n0\n\n\npivot_wider\n100\n0.309\n5.15\n0.307\n0.002\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest\nreplications\nelapsed\nrelative\nuser.self\nsys.self\nuser.child\nsys.child\n\n\n\n\nmelt\n100\n0.005\n1.0\n0.005\n0.000\n0\n0\n\n\npivot_longer\n100\n0.152\n30.4\n0.149\n0.003\n0\n0\n\n\n\n\ndata.table methods are faster than tidyverse and wider to longer is faster than longer to wider."
  },
  {
    "objectID": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#conclusion",
    "href": "posts/2022-03-31-pivoting-data-with-tidyverse-and-datatable-in-r.html#conclusion",
    "title": "Pivoting Data with tidyverse and data.table in R",
    "section": "Conclusion",
    "text": "Conclusion\nI’m hoping that by now you’re more comfortable with pivot_wider/dcast and pivot_longer/melt than you were before. Generally, before you begin reshaping data, you want to know what your resulting data should look like and if you’ll have any duplicate rows. From there, decide if you want to use tidyverse or data.table and then write your code.\n\nAll code used in this article is available here. If you want to see more from me, check out my GitHub or guslipkin.github.io. If you want to hear from me, I’m also on Twitter @guslipkin.\n\nGus Lipkin is a Data Scientist, Business Analyst, and occasional bike mechanic"
  },
  {
    "objectID": "posts/2022-04-14-reordering-bar-and-column-charts-with-ggplot-in-r.html",
    "href": "posts/2022-04-14-reordering-bar-and-column-charts-with-ggplot-in-r.html",
    "title": "Reordering Bar and Column Charts with ggplot2 in R",
    "section": "",
    "text": "Creating the Data\nAs usual, we want to load the tidyverse first since it gives us %>% from magrittr and ggplot2 and everything that comes with it.\n\n\n\nThe dummy data for this post is painting sales. You’re a starving artist who hasn’t taken any days off selling paintings in the past year and you’re trying to start taking a day to yourself every week. Our data has the day of the month (only 28 days per month), the month, the day of the week, and the number of paintings you sold each day. R has a constant built in for the name of the month, month.name, but not one for the days of the week, so we’ll have to make that ourselves. In addition, we start our week on a Sunday.\n\nset.seed(2022)\ndays_of_the_week <- c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \n                      \"Thursday\", \"Friday\", \"Saturday\")\nsales <- data.frame(\"dayOfMonth\" = rep(1:28, 12),\n                    \"month\" = rep(month.name, each = 28),\n                    \"weekday\" = rep(days_of_the_week, 12*4),\n                    \"paintings\" = round(rnorm(28*12, c(sample(1:28, 7)))))\n\nhead(sales, 7)\n\n  dayOfMonth   month   weekday paintings\n1          1 January    Sunday         2\n2          2 January    Monday        20\n3          3 January   Tuesday        13\n4          4 January Wednesday        23\n5          5 January  Thursday        10\n6          6 January    Friday        28\n7          7 January  Saturday         7\n\n\nThe first week’s data shows that we sold the bulk of our paintings on Monday, Wednesday, and Friday, with barely any sales on Sunday and Saturday.\n\n\nGetting the Days in Order\nIn the first chart we make, we want to take the average number of sales per day of the week. To do that, we group_by(weekday) and then summarise the mean(paintings) as a new variable, weekdaySales. In the ggplot chain, we set the chart and axis titles with labs and the color palette with scale_fill_brewer. In aes in geom_bar, we want the weekday on the x axis and weekdaySales on the y axis. To add some color, we set fill = weekday. Outside of the aes but still in geom_bar, we set stat = \"identity\" so that the y axis value is used and show.legend = FALSE to hide the legend.\n\n\n\n\n\n\nNote\n\n\n\nI’ve organized the ggplot() chain so that the items being changed are towards the bottom. The ggplot(), labs(), and scale_fill_brewer() are the same in every example.\nThe only aspects of geom_bar() that will change are the values used for x and y in aes().\n\n\n\nsales %>%\n  group_by(weekday) %>%\n  summarise(weekdaySales = mean(paintings)) %>%\n  ggplot() +\n  labs(title = \"Mean Painting Sales by Day of the Week\",\n       x = \"Day of the Week\",\n       y = \"Mean Painting Sales\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  geom_bar(aes(x = weekday, y = weekdaySales, fill = weekday), \n           stat = \"identity\", show.legend = FALSE)\n\n\n\n\nUnfortunately, our days of the week are not in order. Lots of people would be very sad if Monday came directly after Friday.\n\nsales %>%\n  group_by(weekday) %>%\n  summarise(weekdaySales = mean(paintings)) %>%\n  ggplot() +\n  labs(title = \"Mean Weekly Painting Sales by Day of the Week\",\n       x = \"Day of the Week\",\n       y = \"Mean Painting Sales\") + \n  scale_fill_brewer(palette = \"Set2\") +\n  geom_bar(aes(x = factor(weekday, days_of_the_week), y = weekdaySales, \n               fill = weekday), stat = \"identity\", show.legend = FALSE)\n\n\n\n\nBy changing x = weekday to x = factor(weekday, days_of_the_week), we can get the days in order. The key here is that factor() takes two arguments; a vector of data and a vector defining the levels of the data in order. In this case, days_of_the_week defines the levels in order.\n\n\nAscending or Descending Order\nWhile the graph we just got does the job, we want to see how big the jumps are in order between days of the week. Similarly to getting our days of the week in order, we need to change x = weekday to something else to get our x axis sorted in ascending or descending order. While factor() lets us reorder them by factor levels, reorder allows us to reorder by numeric value. The first argument is the vector we want to change, in this case, weekday, and we want to change it to match the order of the second argument, weekdaySales. In fact, reorder(weekday, weekdaySales) is the same as sales$weekday[order(sales$weekdaySales)].\n\nsales %>%\n  group_by(weekday) %>%\n  summarise(weekdaySales = mean(paintings)) %>%\n  ggplot() +\n  labs(title = \"Mean Weekly Painting Sales in Ascending Order\",\n       x = \"Day of the Week\",\n       y = \"Mean Painting Sales\") + \n  scale_fill_brewer(palette = \"Set2\") +\n  geom_bar(aes(x = reorder(weekday, weekdaySales), y = weekdaySales, \n               fill = weekday), stat = \"identity\", show.legend = FALSE)\n\n\n\n\nThe default order is ascending order, smallest to largest. To reverse the order, we can add a minus sign before the second argument so that it reads reorder(weekday, -weekdaySales).\n\nsales %>%\n  group_by(weekday) %>%\n  summarise(weekdaySales = mean(paintings)) %>%\n  ggplot() +\n  labs(title = \"Mean Weekly Painting Sales in Descending Order\",\n       x = \"Day of the Week\",\n       y = \"Mean Painting Sales\") + \n  scale_fill_brewer(palette = \"Set2\") +\n  geom_bar(aes(x = reorder(weekday, -weekdaySales), y = weekdaySales, \n               fill = weekday), stat = \"identity\", show.legend = FALSE)\n\n\n\n\n\n\nAttempting to Order Within Facets\nBefore we get started on facets, I want to change the data a little so that the examples are a little clearer. By adding some variation into the paintings variable, the differences between days of different months will be more apparent.\n\nsales$paintings <- round(rnorm(28*12, c(sample(1:28, 28*12, replace = TRUE))))\n\nThinking that maybe we’d like to switch up our day off every month, we want to see how our sales change over the course of the year, but still group_by(weekday). However, since we want to know the mean for each weekday for each month, we need to add that so we have group_by(month, weekday). We also add facet_wrap(~ month) to the end of the ggplot chain. Essentially, this means that we want to have a mini plot for each month. In this case, we’ve ordered our y axis so that the days of the week are in order from top to bottom.\n\nsales %>%\n  group_by(month, weekday) %>%\n  summarise(weekdaySales = mean(paintings)) %>%\n  ggplot() +\n  labs(title = \"Daily Painting Sales by Month\",\n       x = \"Mean Painting Sales\",\n       y = \"Day of the Week\") + \n  scale_fill_brewer(palette = \"Set2\") +\n  geom_bar(aes(x = weekdaySales, y = factor(weekday, rev(days_of_the_week)), \n               fill = weekday), stat = \"identity\", show.legend = FALSE) +\n  facet_wrap(~ month)\n\n\n\n\nThat’s all well and good, but what if we want each month ordered in descending order like we had the chart before? We can try changing y = factor() to y = reorder(). We also add scales = \"free_y\" to facet_wrap so that each chart’s y axis is calculated independently. It’s quickly apparent that this doesn’t work the way we were expecting it to. The facets are not sorted properly and all of them are sorted the same way.\n\nsales %>%\n  group_by(month, weekday) %>%\n  summarise(weekdaySales = mean(paintings)) %>%\n  ggplot() +\n  labs(title = \"Daily Painting Sales by Month but not in Descending Order\",\n       x = \"Mean Painting Sales\",\n       y = \"Day of the Week\") + \n  scale_fill_brewer(palette = \"Set2\") +\n  geom_bar(aes(x = weekdaySales, y = reorder(weekday, weekdaySales), \n               fill = weekday), stat = \"identity\", show.legend = FALSE) +\n  facet_wrap(~ month, scales = \"free_y\")\n\n\n\n\nNot sure what happened, we try to investigate. If we re-run the earlier Mean Weekly Painting Sales in Descending Order chart with the new data and grouped by weekday, we find that the y axis in each facet is, in fact, in order with Sunday with the highest sales and Friday with the lowest, but the order hasn’t been recalculated for each month.\n\n\n\n\n\n\n\nOrdering Within Facets\nAs it turns out, there’s a rather simple way to give your facets each their own ordering. The tidytext package provides the functionality with three functions that go hand-in-hand. reorder_within() does the heavy lifting while scale_x_reordered() and scale_y_reordered() make sure that the order set in reorder_within() is respected.\n\n\n\n\n\n\nTip\n\n\n\nJulia Silge writes about reorder_within on her blog here and has some information on its creation and how it came to live in tidytext in the first paragraph of the “Enter reorder_within()” section.\n\n\n\nlibrary(tidytext)\n\nInstead of using factor() and reorder(), we change the y axis argument in aes() to use reorder_within: y = reorder_within(weekday, weekdaySales, month). Our first argument, weekday is the vector we want to reorder. The second argument, weekdaySales is the variable we want to use for reordering. The last argument, month, is the variable we want to use for grouping. The third argument could be a vector of column names if we wanted to group by multiple variables. Like with reorder(), we can set the direction to ascending with a minus sign before the ordering argument, weekdaySales. We keep scales = \"free_y\" in facet_wrap() so that each facet has its own order, and then use scale_y_reordered() to make sure that the new order from reorder_within() is used.\n\nsales %>%\n  group_by(month, weekday) %>%\n  summarise(weekdaySales = mean(paintings)) %>%\n  ggplot() +\n  labs(title = \"Daily Painting Sales in Descending Order by Month\",\n       x = \"Mean Painting Sales\",\n       y = \"Day of the Week\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  geom_bar(aes(x = weekdaySales, \n               y = reorder_within(weekday, weekdaySales, month), \n               fill = weekday), stat = \"identity\", show.legend = FALSE) +\n  facet_wrap(~ month, scales = \"free_y\") +\n  scale_y_reordered()\nsales %>%\n  group_by(month, weekday) %>%\n  summarise(weekdaySales = mean(paintings)) %>%\n  ggplot() +\n  labs(title = \"Daily Painting Sales in Ascending Order by Month\",\n       x = \"Mean Painting Sales\",\n       y = \"Day of the Week\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  geom_bar(aes(x = weekdaySales, \n               y = reorder_within(weekday, -weekdaySales, month), \n               fill = weekday), stat = \"identity\", show.legend = FALSE) +\n  facet_wrap(~ month, scales = \"free_y\") +\n  scale_y_reordered()\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrdering Facets and an Easier Way to Order by Day\nAt this point, you might be staring at your screen in disbelief and thinking, “But Gus, the facets aren’t in order! The months are in alphabetical order and not calendar order!” You’re not wrong. When we were setting the order of y with factor(), we were doing something that we could have done much earlier in our data creation stage. If we redefine the month and weekday variables using the factor(variable, levels) syntax, we don’t need to use y = factor(weekday) to make sure our days are in order. In addition, the facets will now be in calendar order like they appear in the month.name constant. I’ve chosen to reverse the days_of_the_week so that Sunday appears at the top, rather than at the bottom of the y axis.\n\nsales$month <- factor(sales$month, levels = month.name)\nsales$weekday <- factor(sales$weekday, levels = rev(days_of_the_week))\n\nsales %>%\n  group_by(month, weekday) %>%\n  summarise(weekdaySales = mean(paintings)) %>%\n  ggplot() +\n  labs(title = \"Daily Painting Sales in Descending Order by Month\",\n       x = \"Mean Painting Sales\",\n       y = \"Day of the Week\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  geom_bar(aes(x = weekdaySales, y = weekday, fill = weekday), \n           stat = \"identity\", show.legend = FALSE) +\n  facet_wrap(~ month)\n\n\n\n\n\nAll code used in this article is available here. If you want to see more from me, check out my GitHub or guslipkin.github.io. If you want to hear from me, I’m also on Twitter @guslipkin.\n\nGus Lipkin is a Data Scientist, Business Analyst, and occasional bike mechanic"
  },
  {
    "objectID": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html",
    "href": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html",
    "title": "Writing Faster R With Vectorization and the Apply Family",
    "section": "",
    "text": "Link to the Medium post"
  },
  {
    "objectID": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#for-loop",
    "href": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#for-loop",
    "title": "Writing Faster R With Vectorization and the Apply Family",
    "section": "for loop",
    "text": "for loop\nIf you’re familiar with programming, you can probably skip this section.\nA for loop lets you run the same code a specified number of times. The structure generally follows for(x in y) where x represents an item in y. If we think about a shopping basket with some apples, bananas, and carrots, we could write for(food in basket) and food would represent each item in our basket. It would be apples the first time, bananas the second time, and carrots the third time. We could also write it as for(food in 1:length(basket)) where 1:length(basket) is a vector of numbers that counts the items in your basket. Rather than food representing an item in your basket, it represents an index in the vector. In this example, apples are at index 1, bananas at 2, and carrots at 3. for loops are also very flexible and can be used on many data types such as vectors, data.frames, and matrices.\nLet’s say you have a data.frame called basket that has three columns. It has the Food column with the name of the food, the PricePerUnit which has the unit cost for each food, and Quantity which has the number of units of each food in your basket. It looks like this:\n\n\n\nFood\nPricePerUnit\nQuantity\n\n\n\n\nApples\n0.99\n12\n\n\nBananas\n0.19\n6\n\n\nCarrots\n0.49\n2\n\n\n\nAnd it can be recreated with this:\n\nbasket <- data.frame(\"Food\" = c(\"Apples\", \"Bananas\", \"Carrots\"),\n                     \"PricePerUnit\" = c(.99, .19, .49),\n                     \"Quantity\" = c(12, 6, 2))\n\nIf we wanted to get the total cost of everything in our basket, we could iterate over each row multiplying the PricePerUnit and Quantity and adding those to our running totals.\n\n# create the total\ntotal <- 0\n# loop over the data.frame and add the running total\nfor(row in 1:nrow(basket))\n  total <- total + (basket$PricePerUnit[row] * basket$Quantity[row])\ntotal\n\n[1] 14"
  },
  {
    "objectID": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#apply-family",
    "href": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#apply-family",
    "title": "Writing Faster R With Vectorization and the Apply Family",
    "section": "apply family",
    "text": "apply family\nThe apply family is part of base R and very similar to a for loop. Rather than running a set number of times, an apply runs a function on each item in a data.frame, list, vector, or other object that can be applied to. While there are six different functions in the apply family, I’m only going to talk about the three most common; apply, lapply, and sapply.\nThe biggest differences between the three is the types of input that they accept and their output types. apply takes in a data.frame or matrix and has three function arguments. The first argument, x, is the object we’re passing to it. The second argument is a number, either 1 or 2 or c(1, 2), that says if we want the function applied to rows, columns, or both rows and columns, respectively. The last argument is the function call. sapply and lapply are the same, except they don’t have the second argument because they take either a vector or list which don’t have multiple dimensions. Generally speaking, the apply family will return a vector, list, or array of some kind.\nIf we go back to the shopping basket example, we can calculate the total with an apply function. Our first argument is the basket, the second is a 1 because we want to apply to every row, and the last is the function call. We can create the function in the apply call or we can create it earlier and then call it here.\n\n# multiply each PricePerUnit and Quantity and store the resulting vector\nperItemTotal <- apply(basket, 1, function(bskt) {\n  as.numeric(bskt[\"PricePerUnit\"]) * as.numeric(bskt[\"Quantity\"])\n})\n# sum all values in the perItemTotal\nsum(perItemTotal)\n\n[1] 14\n\n\nA quick note on function calls in the apply family:\nIf a function call only has one argument, they can be done in three ways. 1. sapply(X, function(x) { ... }) if function is not predefined 2. sapply(X, function) if function is predefined 3. sapply(X, function(x)) if function is predefined\nOption two is most common for built-in functions such as sum or as.numeric, but can be used with any function."
  },
  {
    "objectID": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#vector-operations",
    "href": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#vector-operations",
    "title": "Writing Faster R With Vectorization and the Apply Family",
    "section": "Vector Operations",
    "text": "Vector Operations\nVector operations are not a function like the apply family or a for loop, but rather a feature of the R language. Instead of operating on a vector one item at a time, R is able to do an operation on the entire vector in one line of code. Back to the basket example again, we know that the per item total is the PricePerUnit and Quantity multiplied together, and then we get the grand total by summing all of those values.\n\n# take the sum of multiplying PerPriceUnit and Quantity to get total cost\nsum(basket$PricePerUnit * basket$Quantity)\n\n[1] 14"
  },
  {
    "objectID": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#for-loop-1",
    "href": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#for-loop-1",
    "title": "Writing Faster R With Vectorization and the Apply Family",
    "section": "for loop",
    "text": "for loop\nfor loops in R should be a last resort. They are much slower compared to the apply family and vectorized code. They may be helpful when each iteration relies on the iteration before it, although then you might want to look into a recursive function if possible. You might find a for loop useful if you need to run the same block of code multiple times or iterate over elements of an object in a non-standard way such as every other item. Any code that can be written with an apply function or a vector operation can be written in a for loop."
  },
  {
    "objectID": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#apply-family-1",
    "href": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#apply-family-1",
    "title": "Writing Faster R With Vectorization and the Apply Family",
    "section": "apply family",
    "text": "apply family\nThe apply family should be used when you want to operate on each element of an object, but treat them individually. This might present as a list with vectors of differing lengths for each item or if you want a specific type of output. Any vector operation can be written as an apply statement, but not all for loops can be converted."
  },
  {
    "objectID": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#vector-operations-1",
    "href": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#vector-operations-1",
    "title": "Writing Faster R With Vectorization and the Apply Family",
    "section": "Vector Operations",
    "text": "Vector Operations\nVector operations are the gold standard. They are fast and can be used in many cases, but not all. Most common use cases will be on vectors or columns of a data.frame. Many base functions such as sum and as.numeric are already vectorized. Many but not all for loops and apply functions can be written as vectorized operations."
  },
  {
    "objectID": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#building-the-input",
    "href": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#building-the-input",
    "title": "Writing Faster R With Vectorization and the Apply Family",
    "section": "Building the input",
    "text": "Building the input\nRather than use the simple shopping basket example from before, I’ve written a small function that takes a data.frame of red, green, and blue values and adds a new column with the corresponding hex code.\n\n# create a vector of the possible hex code values (0-9 and A-F)\nhex <- c(0:9, LETTERS[1:6])\n\n# set the seed\nset.seed(2022)\n# pick the number of rows\nrows <- 10^4\n# create a data.frame of rgb values\ndf <- data.frame(\"red\" = sample(0:255, rows, replace = TRUE), \n                 \"green\" = sample(0:255, rows, replace = TRUE),\n                 \"blue\" = sample(0:255, rows, replace = TRUE))\n\nAnd the resulting data should look like this:\n\n\n\nred\ngreen\nblue\n\n\n\n\n227\n18\n84\n\n\n178\n245\n26\n\n\n205\n219\n176\n\n\n54\n236\n205\n\n\n74\n252\n67\n\n\n195\n116\n122\n\n\n\nWe’ve also created a vector of values that can go in a hex code with numbers 0–9 and letters A-F."
  },
  {
    "objectID": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#creating-the-conversion-function",
    "href": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#creating-the-conversion-function",
    "title": "Writing Faster R With Vectorization and the Apply Family",
    "section": "Creating the conversion function",
    "text": "Creating the conversion function\nI used this website for the math behind my functions. In essence, you divide each number by 16 and round down and the resulting number corresponds to a position in hex. You then take the remainder of the division and get the hex value that that number corresponds to. If our value is 227, then our first hex code is 227/16 would round down to 14 and the remainder would be 3. Because vectors in R start at position 1, we add one to both for 15 and 4. The corresponding values in hex are E and 3 and so the hex pair for 227 is E3."
  },
  {
    "objectID": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#implementing-the-conversion-function",
    "href": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#implementing-the-conversion-function",
    "title": "Writing Faster R With Vectorization and the Apply Family",
    "section": "Implementing the conversion function",
    "text": "Implementing the conversion function\n\nIn a for loop\n\n# iterate over each row in df\nfor(r in 1:nrow(df)) {\n  # get a value for each position in the hex code\n  # first pair\n  h1 <- hex[floor(df$red[r] / 16) + 1]\n  h2 <- hex[df$red[r] %% 16 + 1]\n  \n  # second pair\n  h3 <- hex[floor(df$green[r] / 16) + 1]\n  h4 <- hex[df$green[r] %% 16 + 1]\n\n  # third pair\n  h5 <- hex[floor(df$blue[r] / 16) + 1]\n  h6 <- hex[df$blue[r] %% 16 + 1]\n  \n  # assemble the values using `paste0` and assign it to the `hex` column for \n  # the corresponding row\n  df$hex[r] <- paste0(\"#\", h1, h2, h3, h4, h5, h6)\n}\n\n\n\nIn an apply function\n\ndf <- df[, c(\"red\", \"green\", \"blue\")]\n# create the rgbToHex function that takes a named vector and returns a hex code\nrgbToHex <- function(x) {\n  # get a value for each position in the hex code\n  # first pair\n  h1 <- hex[floor(x[\"red\"] / 16) + 1]\n  h2 <- hex[x[\"red\"] %% 16 + 1]\n  \n  # second pair\n  h3 <- hex[floor(x[\"green\"] / 16) + 1]\n  h4 <- hex[x[\"green\"] %% 16 + 1]\n\n  # third pair\n  h5 <- hex[floor(x[\"blue\"] / 16) + 1]\n  h6 <- hex[x[\"blue\"] %% 16 + 1]\n  \n  # assemble and return the hex code\n  paste0(\"#\", h1, h2, h3, h4, h5, h6)\n}\n# call `rgbToHex` and apply it to each row in df\ndf$hex <- apply(df, 1, rgbToHex)\n\n\n\nIn a vectorized function\n\n# paste the calculated hex codes into the new `hex` column in df\ndf$hex <- paste0(\"#\", \n                 hex[floor(df$red / 16) + 1],\n                 hex[df$red %% 16 + 1],\n                 hex[floor(df$green / 16) + 1],\n                 hex[df$green %% 16 + 1],\n                 hex[floor(df$blue / 16) + 1],\n                 hex[df$blue %% 16 + 1])\n\n\n\nThe results\n\n\n\nred\ngreen\nblue\nhex\n\n\n\n\n227\n18\n84\n#E31254\n\n\n178\n245\n26\n#B2F51A\n\n\n205\n219\n176\n#CDDBB0\n\n\n54\n236\n205\n#36ECCD\n\n\n74\n252\n67\n#4AFC43\n\n\n195\n116\n122\n#C3747A"
  },
  {
    "objectID": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#running-the-benchmark",
    "href": "posts/2022-03-14-writing-faster-r-with-vectorization-and-the-apply-family.html#running-the-benchmark",
    "title": "Writing Faster R With Vectorization and the Apply Family",
    "section": "Running the benchmark",
    "text": "Running the benchmark\nI’ve simplified the for loop and apply implementations a little bit to better match the vectorized function. This way we have a better comparison between the three. Your benchmark results may be a little different because it is a little dependent on your computer.\n\nrows <- 10^4\nhex <- c(0:9, LETTERS[1:6])\n\nset.seed(2022)\ndt <- data.frame(\"red\" = sample(0:255, rows, replace = TRUE), \n                 \"green\" = sample(0:255, rows, replace = TRUE),\n                 \"blue\" = sample(0:255, rows, replace = TRUE))\n\nrbenchmark::benchmark(\n  \"for loop\" = {\n    df <- dt\n    for (r in 1:nrow(df)) {\n      df$hexFor[r] <- paste0(\"#\", \n                             hex[floor(df$red[r] / 16) + 1],\n                             hex[df$red[r] %% 16 + 1],\n                             hex[floor(df$green[r] / 16) + 1],\n                             hex[df$green[r] %% 16 + 1],\n                             hex[floor(df$blue[r] / 16) + 1],\n                             hex[df$blue[r] %% 16 + 1]\n                             )\n    }\n  },\n  \"apply\" = {\n    df <- dt\n    rgbToHex <- function(x) {\n      paste0(\"#\",\n             hex[floor(x[\"red\"] / 16) + 1],\n             hex[x[\"red\"] %% 16 + 1],\n             hex[floor(x[\"green\"] / 16) + 1],\n             hex[x[\"green\"] %% 16 + 1],\n             hex[floor(x[\"blue\"] / 16) + 1],\n             hex[x[\"blue\"] %% 16 + 1]\n             )\n    }\n    df$hexApply <- apply(df, 1, rgbToHex)\n  },\n  \"vector\" = {\n    df <- dt\n    df$hexVector <- paste0(\"#\",\n                           hex[floor(df$red / 16) + 1],\n                           hex[df$red %% 16 + 1],\n                           hex[floor(df$green / 16) + 1],\n                           hex[df$green %% 16 + 1],\n                           hex[floor(df$blue / 16) + 1],\n                           hex[df$blue %% 16 + 1]\n                           )\n  },\n  replications = 10, order = \"relative\"\n) -> benches\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest\nreplications\nelapsed\nrelative\nuser.self\nsys.self\nuser.child\nsys.child\n\n\n\n\nvector\n10\n0.026\n1.000\n0.025\n0.000\n0\n0\n\n\napply\n10\n0.536\n20.615\n0.530\n0.006\n0\n0\n\n\nforloop\n10\n1.569\n60.346\n1.252\n0.317\n0\n0\n\n\n\nThe important column is relative as that shows a comparison between the three with the quickest function given a value of 1. Using an apply function took roughly 20x longer and a for loop roughly 60x longer than using a vectorized function.\n\n\nAll code used in this article is available here. If you want to see more from me, check out my GitHub or guslipkin.github.io. If you want to hear from me, I’m also on Twitter @guslipkin.\n\nGus Lipkin is a Data Scientist, Business Analyst, and occasional bike mechanic"
  },
  {
    "objectID": "posts/2022-08-27-moving-local-sql-to-spark-the-easy-way.html",
    "href": "posts/2022-08-27-moving-local-sql-to-spark-the-easy-way.html",
    "title": "Moving Local Data Pipelines to Spark: The Easy Way with R and Python",
    "section": "",
    "text": "Intro\nAs data people, we know that “the cloud” is usually a server somewhere maybe hosted on Azure, a server in a closet, or maybe a ten year old laptop underneath someone’s desk. When we hear people ask about moving data and processes to the cloud, it’s hard not to think of the Little Green Men from Toy Story worshiping “The Claw”. Within a few short breaths, you’ve been asked to try and move all your current processes to the cloud. It’s a daunting task, but with a few handy tricks, we can make the SQL conversion relatively painless.\n\n\n\nGetting Started\n\n\n\n\n\n\nTip\n\n\n\nI’ve generally tried to keep the R and Python code as similar as possible, but that’s not always the wisest move. If you have any questions, don’t hesitate to reach out.\n\n\nOur first step is to load our packages and connect to spark. We’ll also create a local spark instance to use rather than link to a server. On the R side of things, our core packages are sparklyr and glue while Python is using pyspark and Python 3’s f-String functionality.\n\nRPython\n\n\n\nlibrary(sparklyr)\nlibrary(glue)\n\nsc <- spark_connect(master = \"local\")\n\n\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nsc = SparkSession.builder.getOrCreate()\n\n\n\n\n\n\nPrepping the Data\nAt this point “in the real world” you’ll have loaded your data into the table format of your choosing. Where I work, all of our tables are in delta or parquet formats. For this example, rather than create dummy data for a post like I usually do, I’ve downloaded the Olympic Historical Dataset From Olympedia.org from Kaggle and moved everything to my data folder.\n\nRPython\n\n\n\ntbl_athlete_bio <- \n  spark_read_csv(sc, name = \"athlete_bio\",\n                 path = \"../assets/data/Olympic_Athlete_Bio.csv\")\n\ntbl_athlete_results <- \n  spark_read_csv(sc, name = \"athlete_results\", \n                 path = \"../assets/data/Olympic_Athlete_Event_Results.csv\")\n\ntbl_results <- \n  spark_read_csv(sc, name = \"results\",\n                 path = \"../assets/data/Olympic_Athlete_Event_Results.csv\")\n\ntbl_medal_tally <- \n  spark_read_csv(sc, name = \"medal_tally\",\n                 path = \"../assets/data/Olympic_Games_Medal_Tally.csv\")\n\ntbl_games <- \n  spark_read_csv(sc, name = \"games\",\n                 path = \"../assets/data/Olympics_Games.csv\")\n\ntbl_country <- spark_read_csv(sc, name = \"country\",\n                              path = \"../assets/data/Olympics_Country.csv\")\n\n\n\n\ntbl_athlete_bio = sc.read.csv(\n  \"../assets/data/Olympic_Athlete_Bio.csv\", header = True)\ntbl_athlete_bio.createOrReplaceTempView(\"athlete_bio\")\n\ntbl_athlete_results = sc.read.csv(\n    \"../assets/data/Olympic_Athlete_Event_Results.csv\", header = True)\ntbl_athlete_results.createOrReplaceTempView(\"athlete_results\")\n\ntbl_results = sc.read.csv(\n  \"../assets/data/Olympic_Athlete_Event_Results.csv\", header = True)\ntbl_results.createOrReplaceTempView(\"results\")\n\ntbl_medal_tally = sc.read.csv(\n  \"../assets/data/Olympic_Games_Medal_Tally.csv\", header = True)\ntbl_medal_tally.createOrReplaceTempView(\"medal_tally\")\n\ntbl_games = sc.read.csv(\n  \"../assets/data/Olympics_Games.csv\", header = True)\ntbl_games.createOrReplaceTempView(\"games\")\n\ntbl_country = sc.read.csv(\n  \"../assets/data/Olympics_Country.csv\", header = True)\ntbl_country.createOrReplaceTempView(\"country\")\n\n\n\n\nBy assigning the tables to both an R and Python variable and a name in Spark, we’re able to access the data from both an R and Python context, and a SQL context.\n\nRPython\n\n\n\n# R\nhead(tbl_country)\n# SQL\nsdf_sql(sc, 'SELECT * FROM country LIMIT 6')\n\n\n\n# Source: spark<?> [?? x 2]\n  country_noc country       \n  <chr>       <chr>         \n1 AFG         Afghanistan   \n2 ALB         Albania       \n3 ALG         Algeria       \n4 ASA         American Samoa\n5 AND         Andorra       \n6 ANG         Angola        \n\n\n# Source: spark<?> [?? x 2]\n  country_noc country       \n  <chr>       <chr>         \n1 AFG         Afghanistan   \n2 ALB         Albania       \n3 ALG         Algeria       \n4 ASA         American Samoa\n5 AND         Andorra       \n6 ANG         Angola        \n\n\n\n\n\n\n\n# Python\ntbl_country.show(6)\n# SQL\nsc.sql('SELECT * FROM country LIMIT 6').show()\n\n\n\n+-----------+--------------+\n|country_noc|       country|\n+-----------+--------------+\n|        AFG|   Afghanistan|\n|        ALB|       Albania|\n|        ALG|       Algeria|\n|        ASA|American Samoa|\n|        AND|       Andorra|\n|        ANG|        Angola|\n+-----------+--------------+\nonly showing top 6 rows\n\n\n+-----------+--------------+\n|country_noc|       country|\n+-----------+--------------+\n|        AFG|   Afghanistan|\n|        ALB|       Albania|\n|        ALG|       Algeria|\n|        ASA|American Samoa|\n|        AND|       Andorra|\n|        ANG|        Angola|\n+-----------+--------------+\n\n\n\n\n\n\n\n\n\nWorking With Views\nLet’s say your database administrators (if they aren’t also you) are kind and have done a small amount of data cleaning and you usually access the data from a mount point which provides a view. You pull the schema for the athlete_bio_vw table and get the following SQL that references athlete_bio.\n\nSELECT \n  athlete_id, name, sex, CAST(born AS DATE), \n  CAST(height AS DOUBLE), CAST(weight AS DOUBLE), \n  country, country_noc\nFROM athlete_bio \nWHERE \n  height != \"na\" AND\n  weight != \"na\"\n\nWe can re-create the views three ways. We can use R or Python to recreate the view from scratch by referencing the table variable name, not the internal Spark name. We can also wrap a SQL statement in our language of choice and use the internal Spark name. We then assign this new data frame to tbl_athlete_bio_vw. However, because the new data frame has been assigned as a variable, we no longer have direct and easy access to the view inside further SQL queries, and would have to use R or Python to do any more analysis.\n\nRPython\n\n\n\n# R\ntbl_athlete_bio_vw <-\n  tbl_athlete_bio |> \n    filter(height != \"na\", weight != \"na\") |>\n    mutate(born = as.Date(born), \n           height = as.double(height), \n           weight = as.double(weight)) |>\n    select(athlete_id, name, sex, born, height, weight, country, country_noc)\n# SQL\ntbl_athlete_bio_vw <- sdf_sql(sc, '\n  SELECT \n    athlete_id, name, sex, CAST(born AS DATE), CAST(height AS DOUBLE), \n    CAST(weight AS DOUBLE), country, country_noc\n  FROM athlete_bio \n  WHERE \n    height != \"na\" AND\n    weight != \"na\"')\n\nhead(tbl_athlete_bio_vw)\n\n# Source: spark<?> [?? x 8]\n  athlete_id name                sex    born       height weight country count…¹\n       <int> <chr>               <chr>  <date>      <dbl>  <dbl> <chr>   <chr>  \n1      43737 Andrzej Socharski   Male   1947-08-31    173     72 \" Pola… POL    \n2      50147 Nathalie Wunderlich Female 1971-06-03    170     50 \" Swit… SUI    \n3       5085 Miha Lokar          Male   1935-09-10    182     76 \" Yugo… YUG    \n4     136329 Austin Hack         Male   1992-05-17    203    100 \" Unit… USA    \n5      38633 Tsuneo Ogasawara    Male   1942-07-30    181     80 \" Japa… JPN    \n6      77095 Fulgence Rwabu      Male   1947-11-23    165     51 \" Ugan… UGA    \n# … with abbreviated variable name ¹​country_noc\n\n\n\n\n\n# Python\ntbl_athlete_bio_vw = tbl_athlete_bio.filter('height != \"na\" AND weight != \"na\"'\n    ).select(['athlete_id', 'name', 'sex', col('born').cast('date'), \n              col('height').cast('double'), col('weight').cast('double'), \n              'country', 'country_noc'])\n# SQL\ntbl_athlete_bio_vw = sc.sql('''\n  SELECT \n    athlete_id, name, sex, CAST(born AS DATE), CAST(height AS DOUBLE), \n    CAST(weight AS DOUBLE), country, country_noc\n  FROM athlete_bio \n  WHERE \n    height != \"na\" AND\n    weight != \"na\"''')\n    \ntbl_athlete_bio_vw.show(6)\n\n+----------+-------------------+------+----------+------+------+--------------+-----------+\n|athlete_id|               name|   sex|      born|height|weight|       country|country_noc|\n+----------+-------------------+------+----------+------+------+--------------+-----------+\n|     43737|  Andrzej Socharski|  Male|1947-08-31| 173.0|  72.0|        Poland|        POL|\n|     50147|Nathalie Wunderlich|Female|1971-06-03| 170.0|  50.0|   Switzerland|        SUI|\n|      5085|         Miha Lokar|  Male|1935-09-10| 182.0|  76.0|    Yugoslavia|        YUG|\n|    136329|        Austin Hack|  Male|1992-05-17| 203.0| 100.0| United States|        USA|\n|     38633|   Tsuneo Ogasawara|  Male|1942-07-30| 181.0|  80.0|         Japan|        JPN|\n|     77095|     Fulgence Rwabu|  Male|1947-11-23| 165.0|  51.0|        Uganda|        UGA|\n+----------+-------------------+------+----------+------+------+--------------+-----------+\nonly showing top 6 rows\n\n\n\n\n\nThe third way is a little chaotic, but is actually my preferred method because I can save the queries I want to use as subqueries later on. First we save the query as a string and then use the R {glue} package or f-strings from Python to run the SQL query in the tbl_athlete_bio_qry variable. Because this method lends itself to subqueries so well, I don’t usually use it unless I intend to use the query later on.\nIf you’re not familiar, both glue and f-strings take a regular string and insert the value of the variable inside curly braces {}. For example, if you have a variable named name that has someone’s name. You can use glue or f-strings to write \"hello {name}\" and when this is evaluated, if the name is Gus, it will print “hello Gus.”\n\nRPython\n\n\n\nathlete_bio_vw_qry <- '\n  SELECT \n    athlete_id, name, sex, CAST(born AS DATE), CAST(height AS DOUBLE), \n    CAST(weight AS DOUBLE), country, country_noc\n  FROM athlete_bio \n  WHERE \n    height != \"na\" AND\n    weight != \"na\"'\ntbl_athlete_bio_vw <- sdf_sql(sc, glue(\"{athlete_bio_vw_qry}\"))\n\nhead(tbl_athlete_bio_vw)\n\n# Source: spark<?> [?? x 8]\n  athlete_id name                sex    born       height weight country count…¹\n       <int> <chr>               <chr>  <date>      <dbl>  <dbl> <chr>   <chr>  \n1      43737 Andrzej Socharski   Male   1947-08-31    173     72 \" Pola… POL    \n2      50147 Nathalie Wunderlich Female 1971-06-03    170     50 \" Swit… SUI    \n3       5085 Miha Lokar          Male   1935-09-10    182     76 \" Yugo… YUG    \n4     136329 Austin Hack         Male   1992-05-17    203    100 \" Unit… USA    \n5      38633 Tsuneo Ogasawara    Male   1942-07-30    181     80 \" Japa… JPN    \n6      77095 Fulgence Rwabu      Male   1947-11-23    165     51 \" Ugan… UGA    \n# … with abbreviated variable name ¹​country_noc\n\n\n\n\n\nathlete_bio_vw_qry = '''\n  SELECT \n    athlete_id, name, sex, CAST(born AS DATE), CAST(height AS DOUBLE), \n    CAST(weight AS DOUBLE), country, country_noc\n  FROM athlete_bio \n  WHERE \n    height != \"na\" AND\n    weight != \"na\"'''\ntbl_athlete_bio_vw = sc.sql(f'''{athlete_bio_vw_qry}''')\n\ntbl_athlete_bio_vw.show(6)\n\n+----------+-------------------+------+----------+------+------+--------------+-----------+\n|athlete_id|               name|   sex|      born|height|weight|       country|country_noc|\n+----------+-------------------+------+----------+------+------+--------------+-----------+\n|     43737|  Andrzej Socharski|  Male|1947-08-31| 173.0|  72.0|        Poland|        POL|\n|     50147|Nathalie Wunderlich|Female|1971-06-03| 170.0|  50.0|   Switzerland|        SUI|\n|      5085|         Miha Lokar|  Male|1935-09-10| 182.0|  76.0|    Yugoslavia|        YUG|\n|    136329|        Austin Hack|  Male|1992-05-17| 203.0| 100.0| United States|        USA|\n|     38633|   Tsuneo Ogasawara|  Male|1942-07-30| 181.0|  80.0|         Japan|        JPN|\n|     77095|     Fulgence Rwabu|  Male|1947-11-23| 165.0|  51.0|        Uganda|        UGA|\n+----------+-------------------+------+----------+------+------+--------------+-----------+\nonly showing top 6 rows\n\n\n\n\n\n\n\nSubqueries\nYou probably know that the idea behind subqueries is that it lets you nest operations. The idea behind my third method (I really need a better name for it) is the same. Rather, instead of nesting queries directly, we’re nesting strings which are then evaluated as queries. In this example, we’re going to make another subquery called tbl_athlete_results_qry that contains all columns from the athlete_results Spark table for medal winners. We then want to return all rows from tbl_athlete_bio_vw for people who have won medals.\n\nRPython\n\n\n\nathlete_results_qry <- '\n  SELECT athlete_id \n  FROM athlete_results\n  WHERE medal != \"na\"'\nsdf_sql(sc, glue('\n  SELECT *\n  FROM ({athlete_bio_vw_qry})\n  WHERE athlete_id IN ({athlete_results_qry})')) |>\n  head()\n\n# Source: spark<?> [?? x 8]\n  athlete_id name             sex    born       height weight country    count…¹\n       <int> <chr>            <chr>  <date>      <dbl>  <dbl> <chr>      <chr>  \n1    1700071 Lee Myung-Hwa    Female 1964-09-07    168     57 \" Republi… KOR    \n2      58599 István Kozma     Male   1939-11-27    198    125 \" Hungary\" HUN    \n3      31969 Valter Matošević Male   1970-06-11    194     99 \" Croatia\" CRO    \n4      91237 Volha Puzhevich  Female 1983-03-17    167     43 \" Belarus\" BLR    \n5     104818 Marko Kemppainen Male   1976-07-13    184    100 \" Finland\" FIN    \n6       5695 Lew Beck         Male   1922-04-19    183     75 \" United … USA    \n# … with abbreviated variable name ¹​country_noc\n\n\n\n\n\nathlete_results_qry = '''\n  SELECT athlete_id \n  FROM athlete_results\n  WHERE medal != \"na\"'''\nsc.sql(f'''\n  SELECT *\n  FROM ({athlete_bio_vw_qry})\n  WHERE athlete_id IN ({athlete_results_qry})''').show(6)\n\n+----------+----------------+------+----------+------+------+------------------+-----------+\n|athlete_id|            name|   sex|      born|height|weight|           country|country_noc|\n+----------+----------------+------+----------+------+------+------------------+-----------+\n|   1700071|   Lee Myung-Hwa|Female|1964-09-07| 168.0|  57.0| Republic of Korea|        KOR|\n|     58599|    István Kozma|  Male|1939-11-27| 198.0| 125.0|           Hungary|        HUN|\n|     31969|Valter Matošević|  Male|1970-06-11| 194.0|  99.0|           Croatia|        CRO|\n|     91237| Volha Puzhevich|Female|1983-03-17| 167.0|  43.0|           Belarus|        BLR|\n|    104818|Marko Kemppainen|  Male|1976-07-13| 184.0| 100.0|           Finland|        FIN|\n|      5695|        Lew Beck|  Male|1922-04-19| 183.0|  75.0|     United States|        USA|\n+----------+----------------+------+----------+------+------+------------------+-----------+\nonly showing top 6 rows\n\n\n\n\n\nIf, for whatever reason, we wanted to go even deeper with the subqueries, we could. All we have to do is make sure we create our smallest level queries first then build those out inside the larger queries. Let’s save our query for athletes who have won medals as medal_athlete_qry. It’s important that when we save it, we use glue/f-strings to make sure our string is expanded properly. We can then use medal_athlete_qry as a subquery to count the number of medals won by each country.\n\nRPython\n\n\n\nmedal_athlete_qry <- glue('\n  SELECT *\n  FROM ({athlete_bio_vw_qry})\n  WHERE athlete_id IN ({athlete_results_qry})')\ntbl_country_medal <- sdf_sql(sc, glue('\n  SELECT country, COUNT(country) AS medal_count\n  FROM ({medal_athlete_qry})\n  GROUP BY country\n  ORDER BY medal_count DESC'))\n\nhead(tbl_country_medal)\n\n# Source: spark<?> [?? x 2]\n  country          medal_count\n  <chr>                  <dbl>\n1 \" United States\"        3116\n2 \" Soviet Union\"         1319\n3 \" Germany\"              1005\n4 \" Canada\"                886\n5 \" France\"                803\n6 \" Australia\"             797\n\n\n\n\n\nmedal_athlete_qry = f'''\n  SELECT *\n  FROM ({athlete_bio_vw_qry})\n  WHERE athlete_id IN ({athlete_results_qry})'''\ntbl_country_medal = sc.sql(f'''\n  SELECT country, COUNT(country) AS medal_count\n  FROM ({medal_athlete_qry})\n  GROUP BY country\n  ORDER BY medal_count DESC''')\n                   \ntbl_country_medal.show(6)\n\n+--------------+-----------+\n|       country|medal_count|\n+--------------+-----------+\n| United States|       3116|\n|  Soviet Union|       1319|\n|       Germany|       1005|\n|        Canada|        886|\n|        France|        803|\n|     Australia|        797|\n+--------------+-----------+\nonly showing top 6 rows\n\n\n\n\n\n\n\nAssigning Variable Tables to Spark Tables\nAt this point, you might decide you want to assign your new tables back to Spark, rather than only keeping them in your R or Python environment. This turns out to be a relatively simple operation and we name tbl_athlete_bio_vw to athlete_bio_vw. We can then check the tables available to us to make sure the operation succeeded, and then run a short SQL query to be extra sure it worked.\n\nRPython\n\n\n\ntbl_athlete_bio_vw <- copy_to(sc, tbl_athlete_bio_vw, 'athlete_bio_vw')\n\ndplyr::src_tbls(sc)\n\n[1] \"athlete_bio\"     \"athlete_bio_vw\"  \"athlete_results\" \"country\"        \n[5] \"games\"           \"medal_tally\"     \"results\"        \n\nsdf_sql(sc, 'SELECT * FROM athlete_bio_vw LIMIT 6')\n\n# Source: spark<?> [?? x 8]\n  athlete_id name                sex    born       height weight country count…¹\n       <int> <chr>               <chr>  <date>      <dbl>  <dbl> <chr>   <chr>  \n1      43737 Andrzej Socharski   Male   1947-08-31    173     72 \" Pola… POL    \n2      50147 Nathalie Wunderlich Female 1971-06-03    170     50 \" Swit… SUI    \n3       5085 Miha Lokar          Male   1935-09-10    182     76 \" Yugo… YUG    \n4     136329 Austin Hack         Male   1992-05-17    203    100 \" Unit… USA    \n5      38633 Tsuneo Ogasawara    Male   1942-07-30    181     80 \" Japa… JPN    \n6      77095 Fulgence Rwabu      Male   1947-11-23    165     51 \" Ugan… UGA    \n# … with abbreviated variable name ¹​country_noc\n\n\n\n\n\ntbl_athlete_bio_vw.createOrReplaceTempView('athlete_bio_vw')\n\nsc.sql(\"show tables\").show()\n\n+---------+---------------+-----------+\n|namespace|      tableName|isTemporary|\n+---------+---------------+-----------+\n|         |    athlete_bio|       true|\n|         | athlete_bio_vw|       true|\n|         |athlete_results|       true|\n|         |        country|       true|\n|         |          games|       true|\n|         |    medal_tally|       true|\n|         |        results|       true|\n+---------+---------------+-----------+\n\nsc.sql('SELECT * FROM athlete_bio_vw LIMIT 6').show()\n\n+----------+-------------------+------+----------+------+------+--------------+-----------+\n|athlete_id|               name|   sex|      born|height|weight|       country|country_noc|\n+----------+-------------------+------+----------+------+------+--------------+-----------+\n|     43737|  Andrzej Socharski|  Male|1947-08-31| 173.0|  72.0|        Poland|        POL|\n|     50147|Nathalie Wunderlich|Female|1971-06-03| 170.0|  50.0|   Switzerland|        SUI|\n|      5085|         Miha Lokar|  Male|1935-09-10| 182.0|  76.0|    Yugoslavia|        YUG|\n|    136329|        Austin Hack|  Male|1992-05-17| 203.0| 100.0| United States|        USA|\n|     38633|   Tsuneo Ogasawara|  Male|1942-07-30| 181.0|  80.0|         Japan|        JPN|\n|     77095|     Fulgence Rwabu|  Male|1947-11-23| 165.0|  51.0|        Uganda|        UGA|\n+----------+-------------------+------+----------+------+------+--------------+-----------+\n\n\n\n\n\n\n\nDynamic Queries\nThe last piece that ties all of this together is using variables as different components of your data operations. A relatively common task could be to change dates on a monthly query to filter for the last thirty days. However, I don’t think any Olympians have been born in the last thirty days, probably because the last Olympics was more than thirty days ago. Nevertheless, we can filter for athletes that were born in the last thirty years. In R, the {lubridate} makes the year subtraction a breeze while python-dateutil does all the heavy lifting in Python.\n\nRPython\n\n\n\nlibrary(lubridate)\n\nthirtyYears <- ymd(Sys.Date()) - years(30)\n\nsdf_sql(sc, glue('\n  SELECT *\n  FROM athlete_bio_vw\n  WHERE born >= \"{thirtyYears}\"\n  LIMIT 6'))\n\n# Source: spark<?> [?? x 8]\n  athlete_id name               sex    born       height weight country  count…¹\n       <int> <chr>              <chr>  <date>      <dbl>  <dbl> <chr>    <chr>  \n1     136346 Pedro Pascual      Male   1996-03-15    185     70 \" Unite… USA    \n2     131521 Ana Dascăl         Female 2002-09-12    183     60 \" Roman… ROU    \n3     137091 Saskia Alusalu     Female 1994-04-14    175     64 \" Eston… EST    \n4     134220 Jason Osborne      Male   1994-03-20    178     72 \" Germa… GER    \n5     143729 C. A. Bhavani Devi Female 1993-08-27    168     58 \" India\" IND    \n6     138162 Oskar Svensson     Male   1995-09-07    190     86 \" Swede… SWE    \n# … with abbreviated variable name ¹​country_noc\n\n\n\n\n\nfrom datetime import date\nfrom dateutil.relativedelta import relativedelta\n\nthirtyYears = date.today() - relativedelta(years = 30)\nsc.sql(f'''\n  SELECT *\n  FROM athlete_bio_vw\n  WHERE born >= \"{thirtyYears}\"\n  LIMIT 6''').show()\n\n+----------+------------------+------+----------+------+------+--------------+-----------+\n|athlete_id|              name|   sex|      born|height|weight|       country|country_noc|\n+----------+------------------+------+----------+------+------+--------------+-----------+\n|    136346|     Pedro Pascual|  Male|1996-03-15| 185.0|  70.0| United States|        USA|\n|    131521|        Ana Dascăl|Female|2002-09-12| 183.0|  60.0|       Romania|        ROU|\n|    137091|    Saskia Alusalu|Female|1994-04-14| 175.0|  64.0|       Estonia|        EST|\n|    134220|     Jason Osborne|  Male|1994-03-20| 178.0|  72.0|       Germany|        GER|\n|    143729|C. A. Bhavani Devi|Female|1993-08-27| 168.0|  58.0|         India|        IND|\n|    138162|    Oskar Svensson|  Male|1995-09-07| 190.0|  86.0|        Sweden|        SWE|\n+----------+------------------+------+----------+------+------+--------------+-----------+\n\n\n\n\n\n\n\nDisconnecting From Spark\nBefore we wrap up, let’s disconnect from Spark real quick.\n\nRPython\n\n\n\nspark_disconnect(sc)\n\n\n\n\nsc.stop()\n\n\n\n\n\n\nWrapping Up\nAt the beginning of this post, I made a promise that moving your processes to the cloud doesn’t have to be super painful. It probably still will be painful, but I’m hoping that it’ll now be at most regular painful. We learned that we can either translate our queries to R or Python, run a SQL directly, or save the query as a string, and insert it into later queries where it will be run directly. We can then take our new tables and transfer them back over to Spark so we can reference the tables directly without any shenanigans involving glue or f-strings. Lastly, we can use those same glue and f-string tricks to dynamically change our queries based on R or Python variables.\n\n\nResources\nBoth R and Python have some really great resources out there to help you get started. I highly recommend starting with the documentation for sparklyr and PySpark as each have everything you need to know (besides the info in my post!) to get started.\n\nR: spark.rstudio.com\nPython: PySpark\n\n\nAll code in this article is available here. If you want to see more from me, check out my GitHub or guslipkin.github.io. If you want to hear from me, I’m also on Twitter @guslipkin.\n\nGus Lipkin is a Data Scientist, Business Analyst, and occasional bike mechanic"
  },
  {
    "objectID": "posts/2022-02-23-making-pretty-excel-files-in-r.html",
    "href": "posts/2022-02-23-making-pretty-excel-files-in-r.html",
    "title": "Making Pretty Excel Files in R",
    "section": "",
    "text": "Intro\nIt’s a tale as old as time. Your boss gave you a bunch of Excel files and you painstakingly made a bot that will import and display them in a Shiny dashboard. Proud of your work, you take it to your boss and they say “I don’t know what a ‘Shiny’ is, can’t you just give me one of those Excels back?” openxlsx makes this easy.\nToday we’re going to recreate an existing workbook with openxlsx. To get started, I made a small .xlsx file for us to work with that can be downloaded here. It’s an attendance list and some notes on supplies you’ll need for a company barbecue. Feel free to get familiar with it then come back here when you’re ready.\n\n\n\n\n\n\n\n(a) The attendance sheet\n\n\n\n\n\n\n\n(b) The supplies sheet\n\n\n\n\nFigure 1: A preview of the Attendance and Supplies sheets\n\n\n\n\n\nReading an Existing Workbook\nAs usual, the first thing to do is load our library with library(openxlsx). We can try and load the workbook, the Excel file, with read.xlsx, but for now we’re going to use loadWorkbook. This loads a workbook object and exposes some workbook properties to us rather than just the raw data like with read.xlsx.\n\n# load openxlsx\nlibrary(openxlsx)\n# get the sheet names\nbbq <- loadWorkbook(filePath)\nnames(bbq)\n\n# load the sheets and preview them\nattendance <- readWorkbook(bbq, \"Attendance\")\nhead(attendance)\n\nsupplies <- readWorkbook(bbq, sheet = \"Supplies\")\nhead(supplies)\n\n[1] “Attendance” “Supplies”\n\n\n\n\nName\nRSVPed\nStatus\nFoodPreference\n\n\n\n\n1\nMasud Durga\nTRUE\nNo\nNA\n\n\n2\nStanislav Zillah\nTRUE\nYes\nHotdog\n\n\n3\nJoaquina Aristide\nFALSE\nNA\nNA\n\n\n4\nIuppiter Dieu\nTRUE\nYes\nHotdog\n\n\n5\nHari Evgenios\nTRUE\nYes\nHamburger\n\n\n6\nShaina Gwenaelle\nTRUE\nYes\nHotdog\n\n\n\n\nThe attendance data.frame\n\n\n\n\n\nSupplyType\nQuantity\nPerPackage\nPackagesNeeded\nLeftover\n\n\n\n\n1\nHotdogs\n34\n10\n4\n6\n\n\n2\nHotdog buns\nNA\n8\n5\n6\n\n\n3\nHamburgers\n36\n6\n6\n0\n\n\n4\nHamburger buns\nNA\n8\n5\n4\n\n\n\n\nThe supplies data.frame\n\nBecause we loaded the data as a workbook object, we can use getStyles to load the styles and preview them. Unfortunately, the styles can’t pull conditional formatting and don’t keep track of which cells use which styles. By cross referencing the list of styles and bbq.xlsx, we can identify some styles to use and them assign them each to a variable.\n\n# getStyles and set them appropriately\nstyles <- getStyles(bbq)\nstyles\n\n# doesn't work for colors because those are conditional formatting\nheaderStyle <- styles[[1]]\nnumberStyle <- styles[[7]]\n\nI’ve chosen to only show the first two items from the styles list here.\n[[1]]\nA custom cell style. \n\n Cell formatting: GENERAL \n Font name: Calibri \n Font size: 14 \n Font colour: 1 \n Font decoration: BOLD \n \n\n[[2]]\nA custom cell style. \n\n Cell formatting: GENERAL \n Cell horz. align: center\n\n\n\nCreating a Workbook\nFirst thing we have to do is create a workbook object that we’ll call wb. We can quickly preview it by just typing wb into our chunk or the console.\n\n# create workbook and check contents\nwb <- createWorkbook()\nwb\n\nA Workbook object.\n \nWorksheets:\n No worksheets attached\n\n\nWe’re going to make the Supplies sheet first as it’s a little bit easier. We first add a new worksheet named Supplies to the workbook, then we can write the relevant data to that sheet along with styling our column headers using headerStyle from earlier.\n\n# create the supplies sheet and check for it\naddWorksheet(wb = wb, sheetName = \"Supplies\")\n# write supplies to the worksheet using headerStyle\nwriteData(wb = wb, sheet = \"Supplies\", x = supplies, headerStyle = headerStyle)\n\n\n\nAdding Styles\nAll of the numbers in the Supplies sheet are centered both horizontally and vertically. We can achieve this by adding the numberStyle from before to those cells. rows and cols both start at 2 because row one is the header row and column one is the supply type. rows goes to 5 because there are 4 rows of numbers (remember the header row) and cols goes to 5 because there are 4 columns of numbers. gridExpand = TRUE makes sure that all cell reference combinations possible with rows and cols are used, rather than doing an entire row or column.\n\n# add numberStyle\naddStyle(wb = wb, sheet = \"Supplies\", style = numberStyle, rows = 2:5, \n         cols = 2:5, gridExpand = TRUE)\n\nIf we go back to our data in R, the Quantity for Hotdog buns and Hamburger buns is empty. A bit further back, we see that that’s because those cells were merged with the Quantity of Hotdogs and Hamburgers respectively. We can replicate this using mergeCells. Unlike addStyle, we don’t need to use gridExpand to merge all the cells as it is implied. cols will be 2 for both and we want rows 2 and 3 for Hotdogs and rows 4 and 5 for Hamburgers.\n\n# merge the hamburger and hotdog quantity cells\nmergeCells(wb = wb, sheet = \"Supplies\", cols = 2, rows = 2:3)\nmergeCells(wb = wb, sheet = \"Supplies\", cols = 2, rows = 4:5)\n\nOur last step on this worksheet is to set the column widths to auto. We again need to specify cols and can just do 1:5 so that all columns are affected.\n\n# set column widths to auto\nsetColWidths(wb = wb, sheet = \"Supplies\", cols = 1:5, widths = \"auto\")\n\nOu Supplies sheet is now complete, but we’re not going to write it just yet because we still need to do the Attendance sheet.\nLike before, we first add the Attendance worksheet and write the data to it.\n\n# create the attendance sheet and check for it\naddWorksheet(wb = wb, sheetName = \"Attendance\")\n\n# write attendance to the worksheet using the same headerStyle from before\nwriteData(wb = wb, sheet = \"Attendance\", x = attendance, \n          headerStyle = headerStyle)\n\n\n\n\nConditional Formatting\nThe RSVPed and Status columns each have some conditional formatting. We can reuse the color formatting from the RSVPed column on the Status column, so we’re going to separate the color style from the alignment style. It was easy to see how many rows we have in the Supplies sheet but not here so we’re going to create a new variable that has the rowNumbers. Again, we start on row 2 because of the header row and will end at our last data row plus 1.\n\n# create color styles for rsvp and status\ngoodStyle <- createStyle(fontColour = \"#006100\", bgFill = \"#C6EFCE\")\nbadStyle <- createStyle(fontColour = \"#9C0006\", bgFill = \"#FFC7CE\")\n# create center style for rsvp column\ncenterStyle <- createStyle(halign = \"center\")\n\n# create a variable of row numbers\nrowNumbers <- seq(2, nrow(attendance) + 1, by = 1)\n\nOur first step is to center the values in the RSVPed column with an addStyle. Next, while we could manually color each cell using a for loop, it’s more efficient to use conditionalFormatting. This also has the added bonus of showing in Excel and responding to any changes. The rule argument may be a little strange, and that’s because it must match how the same formatting rule would be written in Excel. In this case, we use our top-leftmost cell as the reference cell in the rule, B2, then we check if it is TRUE or FALSE. When the rule is applied down the cells in column B, the row number will change to match the current row.\n\n# center the column values\naddStyle(wb = wb, sheet = \"Attendance\", \n         style = centerStyle, cols = 2, rows = rowNumbers)\n\n# IF `RSVPed` is TRUE, set it to green. IF FALSE, set it to red\nconditionalFormatting(wb = wb, sheet = \"Attendance\", cols = 2, \n                      rows = rowNumbers, rule = \"B2==TRUE\", style = goodStyle)\nconditionalFormatting(wb = wb, sheet = \"Attendance\", cols = 2, \n                      rows = rowNumbers, rule = \"B2==FALSE\", style = badStyle)\n\nThe Status column is very similar to the RSVPed column, but we add a style for Tentative responses and then need a third conditionalFormatting. The newest part here is in the rule argument. We need to put quotes around Yes so that Excel knows that it is a string and need to use \\\" so that R knows that the quote is part of the string. Once our formatting has been applied, we set the column widths as we did before.\n\n# add style for status and tentative\nmaybeStyle <- createStyle(fontColour = \"#9C6500\", bgFill = \"#FFEB9C\")\n\nconditionalFormatting(wb = wb, sheet = \"Attendance\", cols = 3, \n                      rows = rowNumbers, rule = \"C2==\\\"Yes\\\"\", \n                      style = goodStyle)\nconditionalFormatting(wb = wb, sheet = \"Attendance\", cols = 3, \n                      rows = rowNumbers, rule = \"C2==\\\"No\\\"\", \n                      style = badStyle)\nconditionalFormatting(wb = wb, sheet = \"Attendance\", cols = 3, \n                      rows = rowNumbers, rule = \"C2==\\\"Tentative\\\"\", \n                      style = maybeStyle)\n\n# set column widths to auto\nsetColWidths(wb = wb, sheet = \"Attendance\", cols = 1:4, widths = \"auto\")\n\n\n\n\nWriting the Workbook\nThe last thing we need to do is reorder the worksheets so that Attendance is first because when we created the workbook in R, we created the Supplies worksheet first. Unfortunately, worksheetOrder only supports integer vectors. We can check our worksheet numbers by calling the object again through either the chunk or console. Then we set the worksheet order and save the workbook to an .xlsx file.\n\n# check the workbook sheet order\nwb\n\nA Workbook object.\n \nWorksheets:\n Sheet 1: \"Supplies\"\n \n    Custom column widths (column: width)\n      1: auto, 2: auto, 3: auto, 4: auto, 5: auto \n \n\n Sheet 2: \"Attendance\"\n \n    Custom column widths (column: width)\n      1: auto, 2: auto, 3: auto, 4: auto \n \n\n \n Worksheet write order: 1, 2\n Active Sheet 1: \"Supplies\" \n    Position: 1\n\n# change the order\nworksheetOrder(wb) <- c(2, 1)\n\n# save the workbook\nsaveWorkbook(wb, outputPath, overwrite = TRUE)\n\nIf we open the new file, we can see that they are nearly identical. The biggest difference between the two is that the original used formulas to calculate the values in Supplies.\nIf you did want to take it a step further and use those instead, the writeFormula function is your friend. In any case, I highly encourage everyone to at least skim through the openxlsx documentation here because it has so much to offer to help streamline Excel file generation.\n\nAll the code for this article is available here. If you want to see more from me, check out my GitHub or guslipkin.github.io. If you want to hear from me, I’m also on Twitter @guslipkin.\n\nGus Lipkin is a Data Scientist, Business Analyst, and occasional bike mechanic"
  },
  {
    "objectID": "posts/2022-02-16-grouped-and-stacked-bar-charts-in-r.html",
    "href": "posts/2022-02-16-grouped-and-stacked-bar-charts-in-r.html",
    "title": "Grouped and Stacked Bar Charts in R",
    "section": "",
    "text": "Intro\nSometimes you have a chart that looks like one of these. You have a grouped chart that shows one thing and a stacked chart that shows another. But you really want to show the continent of origin and the condition in one chart.\n\n\n\n\n\n\n\n(a) A grouped bar chart\n\n\n\n\n\n\n\n(b) A stacked bar chart\n\n\n\n\nFigure 1: A grouped bar chart and a stacked bar chart\n\n\nMaybe the chart you want looks a lot like this:\n\n\n\nThe chart that we will learn to build\n\n\n\n\n\nCreating the Data\nFirst we load ggplot2 so we can make our charts. Then we make some data and preview it. set.seed(2022) makes sure that our data is the same every time.\n\nlibrary(ggplot2)\n\nset.seed(2022)\nspecie <- c(rep(\"sorgho\", 6), rep(\"poacee\", 6), \n            rep(\"banana\", 6), rep(\"triticum\", 6))\ncondition <- rep(c(\"normal\" , \"stress\" , \"N2\") , 8)\ncontinent <- rep(c(\"Europe\", \"Africa\", \"Asia\", \"South America\",          \n                   \"North America\", \"Australia\"), 4)\nvalue <- abs(rnorm(24 , 0 , 15))\ndata <- data.frame(specie, condition, continent, value)\nhead(data)\n\n\nCreating the data\n\n\n\n\n\nspecie\ncondition\ncontinent\nvalue\n\n\n\n\n1\nsorgho\nnormal\nEurope\n33.2982559377826\n\n\n2\nsorgho\nstress\nAfrica\n7.4199245988712\n\n\n3\nsorgho\nN2\nAsia\n52.3689859681817\n\n\n4\nsorgho\nnormal\nSouth America\n4.85975695583929\n\n\n5\nsorgho\nstress\nNorth America\n6.84424418650998\n\n\n6\nsorgho\nN2\nAustralia\n6.74199859289553\n\n\n\n\nPreviewing the data\n\n\n\n\nNot Quite Right\nOur first instinct might be to throw both charts together using grid.arrange from the gridextra package. While this does show the information we want, it’s not pretty and doesn’t show the data the way we want it to.\n\none <- ggplot(data) +\n  geom_bar(aes(x = specie, y = value, fill = condition), \n           position = \"dodge\", stat = \"identity\")\ntwo <- ggplot(data) +\n  geom_bar(aes(x = specie, y = value, fill = continent), \n           position = \"stack\", stat = \"identity\")\ngridExtra::grid.arrange(one, two, nrow = 2)\n\n\n\n\nNot sure what to do, we come up with lots of plots that are almost right, but not quite.\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\n\nFigure 2: Three attempts at making a grouped and stacked bar chart\n\n\nThe last one looks like it could be promising. How did we do it?\n\nggplot(data) +\n  geom_bar(aes(x = condition, y = value, fill = continent),\n           position = \"stack\",\n           stat = \"identity\") +\n  facet_wrap(~ specie)\n\nWe use ggplot to set up the pipeline, geom_bar to create the bar chart, and then facet_wrap is what gives us the four separate charts in one, with one mini-chart for each species. If we can move the charts to be side-by-side, we’ll be a lot closer to the desired outcome. We can use facet_grid instead of facet_wrap to accomplish that.\n\nggplot(data) +\n  geom_bar(aes(x = condition, y = value, fill = continent),\n           position = \"stack\",\n           stat = \"identity\") +\n  facet_grid(~ specie)\n\n\n\n\nUsing facet_grid() to show multiple plots next to each other\n\n\n\n\nThis looks much better, but we want it to look like one cohesive plot, rather than four smaller plots.\n\n\n\nThe Final Product\nI’m going to show you the code that does it, then walk through it so everything makes sense.\n\nggplot(data) +\n  geom_bar(aes(x = condition, y = value, fill = continent),\n           position = \"stack\",\n           stat = \"identity\") +\n  facet_grid(~ specie, switch = \"x\") +\n  theme(strip.placement = \"outside\",\n        strip.background = element_rect(fill = NA, color = \"white\"),\n        panel.spacing = unit(-.01,\"cm\"))\n\n\n\n\nThe graph that we came here for\n\n\n\n\nThis looks pretty good and is exactly what we wanted. Like the charts before, we get 90% of the way there with ggplot, geom_bar, and facet_grid. The additions here are the switch = \"x\" argument in facet_grid, which moves the group panel with the species from the top of the chart to the bottom. Moving the strip.placement outside makes sure that the conditions are listed between the species and the chart. Making strip.background empty with a white border allows the group panel with the species to blend in with the white background of the chart. Lastly, changing the panel.spacing to -.01 removes the small gap between each panel so that the chart appears to be one cohesive unit.\n\nThe code for all the charts in this article is available here. If you want to see more from me, check out my GitHub or guslipkin.github.io. If you want to hear from me, I’m also on Twitter @guslipkin.\n\nGus Lipkin is a Data Scientist, Business Analyst, and occasional bike mechanic"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi There 👋",
    "section": "",
    "text": "I’m Gus, a Business Analyst with a few years’ experience. I’d like to show you what I do.\nYou can also find me on Twitter @guslipkin or on Medium also @guslipkin."
  },
  {
    "objectID": "index.html#data-focused-projects",
    "href": "index.html#data-focused-projects",
    "title": "Hi There 👋",
    "section": "Data Focused Projects",
    "text": "Data Focused Projects"
  },
  {
    "objectID": "index.html#economic-analysis-focused-projects",
    "href": "index.html#economic-analysis-focused-projects",
    "title": "Hi There 👋",
    "section": "Economic Analysis Focused Projects",
    "text": "Economic Analysis Focused Projects"
  },
  {
    "objectID": "index.html#software-development",
    "href": "index.html#software-development",
    "title": "Hi There 👋",
    "section": "Software Development",
    "text": "Software Development"
  },
  {
    "objectID": "index.html#other-projects",
    "href": "index.html#other-projects",
    "title": "Hi There 👋",
    "section": "Other Projects",
    "text": "Other Projects"
  },
  {
    "objectID": "index.html#consulting",
    "href": "index.html#consulting",
    "title": "Hi There 👋",
    "section": "Consulting",
    "text": "Consulting"
  },
  {
    "objectID": "capstone.html",
    "href": "capstone.html",
    "title": "Patient Readmission Rates with Tallahassee Memorial Healthcare",
    "section": "",
    "text": "Previous\n\n\n Next"
  },
  {
    "objectID": "capstone.html#demographics",
    "href": "capstone.html#demographics",
    "title": "Patient Readmission Rates with Tallahassee Memorial Healthcare",
    "section": "Demographics",
    "text": "Demographics"
  },
  {
    "objectID": "capstone.html#story",
    "href": "capstone.html#story",
    "title": "Patient Readmission Rates with Tallahassee Memorial Healthcare",
    "section": "Story",
    "text": "Story\n\nAll stories are fictional and creations of the author"
  },
  {
    "objectID": "capstone.html#discharge-survey",
    "href": "capstone.html#discharge-survey",
    "title": "Patient Readmission Rates with Tallahassee Memorial Healthcare",
    "section": "Discharge Survey",
    "text": "Discharge Survey\nPlease answer this survey based on the above narrative\n\n\nDuring this hospital stay, did you need help from nurses or other hospital staff in getting to the bathroom or in using a bed pan?\n\nYes No  \n\nIn general, how would you rate your overall health?\n\nPoor Fair Good Very Good Excellent  \n\nBefore giving you any new medicine, how often did hospital staff tell you what the medicine was for?\n\nNever Rarely Sometimes Often Always  \n\nWhen I left the hospital, I clearly understood the purpose for taking each of my medications?\n\nNever Seldom Sometimes Often Always  \n\nDuring this hospital stay, were you admitted to this hospital through the emergency room?\n\nYes No   Submit your survey Reset your survey"
  },
  {
    "objectID": "capstone.html#your-results",
    "href": "capstone.html#your-results",
    "title": "Patient Readmission Rates with Tallahassee Memorial Healthcare",
    "section": "Your Results",
    "text": "Your Results\n\nYour results will be here!"
  },
  {
    "objectID": "about.html#a-brief-history",
    "href": "about.html#a-brief-history",
    "title": "About Me",
    "section": "A Brief History",
    "text": "A Brief History\n\n\nI was born and raised in Massachusetts about twenty minutes west of Boston. In elementary school, I spent my summers at circus camp where I learned to ride a unicycle, walk on stilts, a variety of other skills generally requiring a large soft pad at the bottom, and failing to figure out how to juggle. In fourth grade, I got so frustrated that I did a project on juggling so that I knew exactly what to do, but I’ve still never been able to get my hands to move the right way. During the fall and winter months, I did ballet and danced in a production of The Nutcracker. It was during our downtime then that I first learned to solve a Rubik’s cube. In middle school, I took part in an archery club after school and eventually began to mentor younger students and carried that on through high school. My junior year of high school, I help start the VEX robotics team and went to World Championships. I also became involved in TV production and joined the local channel.\n\n\n\n  A backstage photo from The Nutcracker (2009)\n\n\n\n\n\n\nAfter graduating high school in 2015, I launched the archery program at a summer camp in Canada and designed and built a modular Gaga pit. (Gaga is sort of like a dodgeball battle-royale that you may recognize from an episode of Bob’s Burgers). I spent the next few years working at a Starbucks where I won my store’s Barista Championship in 2018 and 2019. During my time with the company, I quickly learned all the recipes and became one of the fastest and most reliable baristas in the store. Because of my knowledge and reliablity, I was asked to become a Barista Trainer and helped train baristas, supervisors, and store managers from across the district.\n\n\n\n  Working at Starbucks (2017)\n\n\n\n I attended Florida Poly from Fall 2018 and graduated in May 2022. The rest, as they say, is history. You can learn more about my time at Florida Poly and projects here. You can learn more about what I’ve been up to since then by taking a peek at my LinkedIn, looking at my résumé, or just asking me."
  },
  {
    "objectID": "about.html#selected-interests",
    "href": "about.html#selected-interests",
    "title": "About Me",
    "section": "Selected Interests",
    "text": "Selected Interests\n\nBiking 🚴‍♂️🚵‍♂️\nMany years ago, there was a summer that I was on my bike so much that I didn’t even take my helmet off for meals. While my passion for biking certainly hasn’t waned, I don’t have as much time as I did when my biggest worries were making it home before dark. Until I moved to Florida, I volunteered for many years with the Pan-Mass Challenge, first as a tire pumper and then a mechanic as I developed my skills. More recently, I started a small mountain biking YouTube channel that I update when I can. I also participated in the 2021 Horrible Hundred and it was exactly as horrible as you might imagine.\n\n\nSpeedsolving\nWhen someone sees you with a Rubik’s cube and asks if you can solve it, they almost never expect to be told, “Give me 15 seconds” and then presented with a solved cube. I have a relatively small collection of puzzles and enjoy the zen mindset that can be found from practicing. I’m rather pleased with my competition results and have some of my solves on camera on a YouTube channel. I’m hoping to get out to competitions a bit more so I can update everyone with all the progress I’ve made. Within the community, I served on the WCA Communications Committee for a brief period, have helped staff a variety of competitions, and even organized my own competition in my hometown called Framingham Frozen Fingers."
  },
  {
    "objectID": "about.html#recommendations-from-friends",
    "href": "about.html#recommendations-from-friends",
    "title": "About Me",
    "section": "Recommendations from Friends",
    "text": "Recommendations from Friends\n\n\n“Gus is one of the most dependable and consistent people I have ever known.” – Melia R\n\n\n“You can always rely on Gus to burn his food” – Izzy Z\n\n\n“This is a guy that knows the guy you need for whatever it is you need” – Tucker M-S\n\n\n“Lemme think about it” – Macy L\n\n\n“Gus Lipkin, in the many the years I’ve known him, has enough resourcefulness and preparation skills in him to do the work of a dozen people” – Chris H"
  },
  {
    "objectID": "presentations/cat_simulator_2019.html",
    "href": "presentations/cat_simulator_2019.html",
    "title": "Cat Simulator 2019",
    "section": "",
    "text": "🐱 Print a copy"
  },
  {
    "objectID": "presentations/trader_joes_cultural_marketing_plan.html",
    "href": "presentations/trader_joes_cultural_marketing_plan.html",
    "title": "Trader Joe’s Cultural and Marketing Plan",
    "section": "",
    "text": "🛒 Print a copy"
  },
  {
    "objectID": "presentations/spreadsheet_guide.html",
    "href": "presentations/spreadsheet_guide.html",
    "title": "Gus’ Good Spreadsheet Guide",
    "section": "",
    "text": "💾 Print a copy"
  },
  {
    "objectID": "presentations/tutoring_at_poly.html",
    "href": "presentations/tutoring_at_poly.html",
    "title": "Improving Tutoring at Florida Poly",
    "section": "",
    "text": "📚 Print a copy"
  },
  {
    "objectID": "presentations/disney_world_ride_wait_times.html",
    "href": "presentations/disney_world_ride_wait_times.html",
    "title": "Disney World ride wait times",
    "section": "",
    "text": "🏰 Print a copy"
  },
  {
    "objectID": "presentations/covid_time_series_gis.html",
    "href": "presentations/covid_time_series_gis.html",
    "title": "Investigating a relationship between climate variables and the spread of COVID-19",
    "section": "",
    "text": "🦠 Print a copy"
  },
  {
    "objectID": "presentations/covid_time_series_gis.html#watch-me-present",
    "href": "presentations/covid_time_series_gis.html#watch-me-present",
    "title": "Investigating a relationship between climate variables and the spread of COVID-19",
    "section": "Watch me present",
    "text": "Watch me present"
  },
  {
    "objectID": "dewey.html",
    "href": "dewey.html",
    "title": "Gus Lipkin's Awesome Website",
    "section": "",
    "text": "Install Process\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"guslipkin/dewey\")\n\n\n\nregsearch\n(data, dependent, independent, minvar = 1, maxvar, family, topN = 0, interactions = FALSE, multi = FALSE, ...)\nAn exhaustive search regression built on base R\n\n\nifelsedata\n(x, y, arg = NULL, matchCols = FALSE)\nFast data.frame comparisons at the cell level\n\n\ndiffFill\n(x, lag = 1, differences = 1, ...)\nA wrapper for the base diff function that returns a data.frame of the same length as the input. Allows for vector input for lag or differences.\n\n\nlagMultiple\n(x, k = 1)\nAppropriately lags an input variable and returns a data.frame of the same length as the input. Allows for vector input for k."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blog",
    "section": "",
    "text": "R\n\n\nR: sparklyr\n\n\nPython\n\n\nPython: PySpark\n\n\nSpark\n\n\n \n\n\n\n\nAug 27, 2022\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nR: Tidy Tuesday\n\n\n \n\n\n\n\nMay 12, 2022\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nR: tidyverse\n\n\nR: ggplot2\n\n\nR: tidytext\n\n\n \n\n\n\n\nApr 14, 2022\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nR: tidyverse\n\n\nR: data.table\n\n\n \n\n\n\n\nMar 31, 2022\n\n\n39 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nR: apply\n\n\n \n\n\n\n\nMar 14, 2022\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nR: openxlsx\n\n\nExcel\n\n\n \n\n\n\n\nFeb 23, 2022\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nR: tidyverse\n\n\nR: ggplot2\n\n\n \n\n\n\n\nFeb 16, 2022\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  }
]